{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e02fad",
   "metadata": {},
   "source": [
    "### ğŸ“˜ Talleres de IngenierÃ­a de Datos con Pandas y Polars ğŸ¼ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db83f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ğŸ‘¨â€ğŸ’» Autor: Brayan Neciosup  \n",
    "ğŸ“ Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "ğŸ”— LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "ğŸ’» GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "ğŸ“š Serie: Fundamentos de Pandas y Polars   \n",
    "ğŸ““ Estos talleres constarÃ¡n de 3 niveles (BÃ¡sico-Intermedio-Avanzado)   \n",
    "ğŸ” AbarcarÃ¡ temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "ğŸ“ Cada ejercicio presenta su enunciado, dataset, resultado esperado y soluciÃ³n.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cd600",
   "metadata": {},
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACIÃ“N DE DATOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90306f5b",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1a1a3",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51d9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. DetecciÃ³n de valores nulos en columnas principales\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Identifica cuÃ¡ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "âœï¸ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = sns.load_dataset(\"titanic\")\n",
    "# df_uno.head()\n",
    "# df_uno[[\"age\",\"embarked\",\"deck\"]].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n",
    "\n",
    "\"\"\"\n",
    "2. EliminaciÃ³n de filas duplicadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Elimina las filas duplicadas y conserva solo la primera apariciÃ³n de cada registro.\n",
    "âœï¸ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"MarÃ­a\",\"Pedro\",\"Pedro\",\"SofÃ­a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pd.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "# df_dos.shape[0] ## â¡ï¸ Cantidad de datos: 7\n",
    "df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"],keep=\"first\",inplace=True)\n",
    "# df_dos.head()\n",
    "df_dos.shape[0] ## â¡ï¸ Cantidad de datos: 5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: PENGUINS\n",
    "ğŸ—’ï¸ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "âœï¸ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = sns.load_dataset(\"penguins\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 2\n",
    "media_bill_length_mm = float(np.mean(df_tres[\"bill_length_mm\"].dropna()).round(2))\n",
    "df_tres.fillna({\"bill_length_mm\":media_bill_length_mm},inplace=True)\n",
    "df_tres[\"bill_length_mm\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3644744",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f88ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. DetecciÃ³n de valores nulos en columnas principales\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Identifica cuÃ¡ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "âœï¸ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_uno.head()\n",
    "df_uno.null_count()\n",
    "\n",
    "\"\"\"\n",
    "2. EliminaciÃ³n de filas duplicadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Elimina las filas duplicadas y conserva solo la primera apariciÃ³n de cada registro.\n",
    "âœï¸ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"MarÃ­a\",\"Pedro\",\"Pedro\",\"SofÃ­a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pl.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "df_dos = df_dos.unique(keep=\"first\")\n",
    "df_dos.head()\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: PENGUINS\n",
    "ğŸ—’ï¸ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "âœï¸ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].null_count() ## â¡ï¸ Cantidad Nulos: 2\n",
    "media_bill_length_mm = df_tres[\"bill_length_mm\"].mean().__round__(2)\n",
    "media_bill_length_mm\n",
    "df_tres = df_tres.with_columns(\n",
    "    pl.col(\"bill_length_mm\").fill_null(media_bill_length_mm).alias(\"bill_length_mm\")\n",
    ")\n",
    "# df_tres.head()\n",
    "df_tres[\"bill_length_mm\"].null_count() ## â¡ï¸ Cantidad Nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87889e12",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f9c68",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "4. ImputaciÃ³n condicional de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "âœï¸ Resultado esperado: columna age sin valores nulos, imputada segÃºn clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = sns.load_dataset(\"titanic\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "edad_promedio_por_clase = df_cuatro.groupby(\"pclass\",as_index=False,observed=True)[\"age\"].mean().round(2)\n",
    "edad_promedio_por_clase\n",
    "# df_cuatro[\"age\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "\n",
    "df_cuatro_clean = df_cuatro.copy()\n",
    "df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1] = df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==1')[\"age\"][0])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==2] = df_cuatro[df_cuatro[\"pclass\"]==2].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==2')[\"age\"][1])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==3] = df_cuatro[df_cuatro[\"pclass\"]==3].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query(\"pclass==3\")[\"age\"][2])\n",
    "})\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==2].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==3].isnull().sum() ## Cantidad: 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. DetecciÃ³n de outliers usando IQR\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica los valores de ventas que son outliers segÃºn el rango intercuartÃ­lico (IQR).\n",
    "âœï¸ Resultado esperado: listado de los productos que presentan valores anÃ³malos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "# df_cinco.describe()\n",
    "q1_ventas = float(np.quantile(df_cinco[\"ventas\"],0.25))\n",
    "q3_ventas = float(np.quantile(df_cinco[\"ventas\"],0.75))\n",
    "iqr_ventas = q3_ventas-q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "df_cinco_outliers = df_cinco[(df_cinco[\"ventas\"]<lower_bound_ventas) | (df_cinco[\"ventas\"]>upper_bound_ventas)]\n",
    "# df_cinco_outliers.head()\n",
    "df_cinco_outliers.shape[0] ## CANTIDAD DE OUTLIERS EN COLUMNA VENTAS: 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. EliminaciÃ³n selectiva de duplicados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "âœï¸ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = sns.load_dataset(\"diamonds\")\n",
    "df_seis.head()\n",
    "# df_seis.shape[0] ## 53940 DATOS\n",
    "# df_seis[\"carat\"].duplicated().sum() ## 53 667 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis[\"price\"].duplicated().sum() ## 42 338 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis.drop_duplicates(subset=[\"carat\",\"price\"],inplace=True)\n",
    "# df_seis.shape[0] ## 28988 DATOS DESPUES DE REMOVER DUPLICADOS\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaciÃ³n\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolaciÃ³n lineal.\n",
    "âœï¸ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_siete = {\n",
    "    \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    "    \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pd.DataFrame(diccionario_siete)\n",
    "df_siete_interpolado = df_siete.copy()\n",
    "df_siete_interpolado = df_siete_interpolado.interpolate(method=\"linear\")\n",
    "df_siete_interpolado.head()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Penguins\n",
    "ğŸ—’ï¸ Enunciado: Calcula el nÃºmero de registros que tienen valores nulos simultÃ¡neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "âœï¸ Resultado esperado: un nÃºmero entero que indique cuÃ¡ntos registros cumplen esta condiciÃ³n.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = sns.load_dataset(\"penguins\")\n",
    "# df_ocho.head()\n",
    "df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].isnull().sum() ## â¬…ï¸ Cantidad de datos nulos en ambas columnas (bill_length_mm:2 - bill_depth_mm: 2).\n",
    "df_ocho_nulos = df_ocho[(df_ocho[\"bill_length_mm\"].isnull()==True) & (df_ocho[\"bill_depth_mm\"].isnull()==True)]\n",
    "df_ocho_nulos.shape[0] ## â¬…ï¸ Cantidad de valores nulos simultaneos en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo ğŸ‘.\n",
    "df_example = pd.DataFrame(data=[[1,None],[None,2],[None,None]],columns=[\"A\",\"B\"])\n",
    "# df_example.head() ## â¬…ï¸ Como podremos observar hay un match en dos registros de la fila A y B que son nulos.\n",
    "cantidad_nulos = df_example[(df_example[\"A\"].isnull()==True) & (df_example[\"B\"].isnull()==True)]\n",
    "cantidad_nulos.shape[0] ## â¬…ï¸ Cantidad de valores nulos simultaneos en ambas columnas: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d7d0b",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bbcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "4. ImputaciÃ³n condicional de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "âœï¸ Resultado esperado: columna age sin valores nulos, imputada segÃºn clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].null_count() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "def llenar_nulos_pclass_edad(dataframe):\n",
    "    df = dataframe\n",
    "    edad_promedio_por_clase = df_cuatro.group_by(\"pclass\").agg(\n",
    "        pl.col(\"age\").drop_nulls().mean().round(2).alias(\"avg_age_pclass\")\n",
    "    )\n",
    "    \n",
    "    for pclass,media in edad_promedio_por_clase.iter_rows():\n",
    "        df = df.with_columns(\n",
    "            pl.when(\n",
    "                (pl.col(\"pclass\")==pclass) & (pl.col(\"age\").is_null())\n",
    "            ).then(pl.lit(media))\n",
    "            .otherwise(pl.col(\"age\")).alias(\"age\")\n",
    "        )\n",
    "    return df\n",
    "df_cuatro_clean = llenar_nulos_pclass_edad(df_cuatro)\n",
    "df_cuatro_clean[[\"age\"]].null_count() ## â¡ï¸ Cantidad de datos nulos: 0 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. DetecciÃ³n de outliers usando IQR\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica los valores de ventas que son outliers segÃºn el rango intercuartÃ­lico (IQR).\n",
    "âœï¸ Resultado esperado: listado de los productos que presentan valores anÃ³malos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "q1_ventas = df_cinco[\"ventas\"].quantile(0.25)\n",
    "q1_ventas\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. EliminaciÃ³n selectiva de duplicados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "âœï¸ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_seis.head()\n",
    "df_seis = df_seis.unique(subset=[\"carat\",\"price\"],keep=\"first\")\n",
    "df_seis.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaciÃ³n\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolaciÃ³n lineal.\n",
    "âœï¸ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_siete  = {\n",
    " \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pl.DataFrame(diccionario_siete)\n",
    "df_siete = df_siete.select(pl.col(\"fecha\"),pl.col(\"temperatura\").interpolate().alias(\"temperatura\"))\n",
    "df_siete.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Penguins\n",
    "ğŸ—’ï¸ Enunciado: Calcula el nÃºmero de registros que tienen valores nulos simultÃ¡neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "âœï¸ Resultado esperado: un nÃºmero entero que indique cuÃ¡ntos registros cumplen esta condiciÃ³n.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_ocho.head()\n",
    "# df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].null_count() ## â¡ï¸ Cantidad de Valores Nulos\n",
    "nulos_consecutivos = df_ocho.filter(\n",
    "    (pl.col(\"bill_length_mm\").is_null() & pl.col(\"bill_depth_mm\").is_null())\n",
    ")\n",
    "# nulos_consecutivos.head() ## â¬…ï¸ Verificamos nulos simulares en ambas columnas\n",
    "# nulos_consecutivos.shape[0] ## â¬…ï¸ Cantidad de nulos simulares en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo ğŸ‘.\n",
    "df_example = pl.DataFrame(data=[[1,None],[None,2],[None,None]],schema=[\"A\",\"B\"],orient=\"row\")\n",
    "# df_example.head() ## â¬…ï¸ Verificamos que existen nulos en columnas similares\n",
    "nulos_consecutivos_2 = df_example.filter(\n",
    "    (pl.col(\"A\").is_null() & pl.col(\"B\").is_null())\n",
    ")\n",
    "# nulos_consecutivos_2.head() ## â¬…ï¸ Verificamos nulos simulares en ambas columnas\n",
    "nulos_consecutivos_2.shape[0] ## â¬…ï¸ Cantidad de nulos simulares en ambas columnas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db608e",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a022a",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a91a88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ciudad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ciudad\n",
       "0   Lima\n",
       "1   Lima\n",
       "2   Lima\n",
       "3   Lima"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. WinsorizaciÃ³n de outliers\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica winsorizaciÃ³n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "âœï¸ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "ğŸ’¡ La winsorinizaciÃ³n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 12, 15, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pd.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## â¡ï¸ Quartil 5%: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## â¡ï¸ Quartil 95%: 170.0\n",
    "# limite_5_pct_superior\n",
    "condiciones_valores = [\n",
    "    (df_nueve[\"nota\"]<limite_5_pct_inferior), ## Condicina que, si el valor es menor al lÃ­mite 5% inferior\n",
    "    (df_nueve[\"nota\"]>limite_5_pct_superior)  ## Condicina que, si el valor es mayor al lÃ­mite 95% superior\n",
    "]\n",
    "values = np.array([limite_5_pct_inferior,limite_5_pct_superior]) ## Se establece los valores de las condiciones\n",
    "df_nueve_winzorizacion = df_nueve.copy()\n",
    "df_nueve_winzorizacion[\"nota_winsorizacion\"] = np.select(condiciones_valores,values,default=df_nueve[\"nota\"])\n",
    "df_nueve_winzorizacion.head(7)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "10. DetecciÃ³n de inconsistencias de tipado en columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica las filas con tipos incorrectos  y conviÃ©rtelas al tipo correcto.\n",
    "âœï¸ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "df_diez[\"ID\"] = df_diez[\"ID\"].astype(dtype=\"int\") ## Casteamos el tipo de dato a int\n",
    "df_diez[\"Venta\"] = df_diez[\"Venta\"].astype(dtype=\"float\") ## Casteamos el tipo de dato a float\n",
    "# df_diez.head()\n",
    "df_diez.dtypes ##  âœ… Verificamos los tipos de dato correctamente casteados.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "11. IdentificaciÃ³n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogrÃ¡ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "âœï¸ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedro\"]\n",
    "}\n",
    "\n",
    "import difflib ## MÃ³dulo que permite encontrar diferencias/similitudes en secuencias.\n",
    "\n",
    "nombres = [i.replace(' ','') for i in diccionario_once[\"cliente\"]] ## List comprenhension que limpiar espacios en blanco\n",
    "\n",
    "sospechosos_duplicados = {} ## Diccionario para almacenar duplicados sospechosos\n",
    "\n",
    "for nombre in nombres: ## Iteramos en a lista anterior.\n",
    "    ## .get_close_matches() Retorna las palabras similares a la primera que encuentre.\n",
    "    similares = difflib.get_close_matches(word=nombre,possibilities=nombres,n=3,cutoff=0.8)\n",
    "    ## word: Palabra a encontrar\n",
    "    ## possibilities: Lista de palabras similares\n",
    "    ## n: Cantidad de coincidencias\n",
    "    ## cutoff: Valor de similitud (Como requerimos las que sean casi similar usamos un 0.8=80%)\n",
    "    sospechosos_duplicados[nombre] = similares ## Almacenamos en el diccionario\n",
    "sospechosos_duplicados ## Imprimimos el diccionario\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. ValidaciÃ³n y limpieza de rangos vÃ¡lidos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Valida que la columna \"age\" estÃ© en un rango lÃ³gico (0 a 100 aÃ±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "âœï¸ Resultado esperado: columna age sin valores invÃ¡lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = sns.load_dataset(\"titanic\")\n",
    "# df_doce.shape[0] ## â¡ï¸ Cantidad de datos originales: 891\n",
    "df_doce_clean = df_doce.query('age>0 and age<100') ## âœ… Filtramos la informaciÃ³n y almacenamos en un nuevo dataset.\n",
    "df_doce_clean.shape[0] ## â¡ï¸ Cantidad de datos originales: 714\n",
    "# df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n de categorÃ­as inconsistentes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta y unifica las categorÃ­as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcciÃ³n de errores).\n",
    "âœï¸ Resultado esperado: Dataset con categorÃ­as Ãºnicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "\n",
    "categorias_validas = [\"Lima\"] ## â¡ï¸ Palabras vÃ¡lidas para su estandarizaciÃ³n\n",
    "\n",
    "def normalizar_ciudad(valor):\n",
    "    match = difflib.get_close_matches(valor, categorias_validas, n=1, cutoff=0.6)\n",
    "    return match[0] if match else valor ## Verifica que sÃ­ la palabra ingresada\n",
    "                                        ## tiene una palabra vÃ¡lida similar, entonces\n",
    "                                        ## retorna esa palabra similar en una lista (accedemos a su posiciÃ³n).\n",
    "df_trece = pd.DataFrame(diccionario_trece)\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].str.title()\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].apply(normalizar_ciudad)\n",
    "df_trece.head()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f804d90",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. WinsorizaciÃ³n de outliers\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica winsorizaciÃ³n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "âœï¸ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "ğŸ’¡ La winsorinizaciÃ³n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 11, 75, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pl.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head(7)\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## â¡ï¸ LÃ­mite 5% inferior: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## â¡ï¸ LÃ­mite 5% superior: 170.0 \n",
    "# limite_5_pct_superior\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"nota\")<limite_5_pct_inferior\n",
    "    ).then(pl.lit(limite_5_pct_inferior))\n",
    "    .when(\n",
    "        pl.col(\"nota\")>limite_5_pct_superior\n",
    "    ).then(pl.lit(limite_5_pct_superior))\n",
    "    .otherwise(pl.col(\"nota\")).alias(\"nota_estandarizada\")\n",
    ")\n",
    "df_nueve.head(7)\n",
    "\n",
    "\"\"\"\n",
    "10. DetecciÃ³n de inconsistencias de tipado en columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica las filas con tipos incorrectos  y conviÃ©rtelas al tipo correcto.\n",
    "âœï¸ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pl.DataFrame(diccionario_diez,schema={\"ID\":pl.Object,\"Venta\":pl.Object})\n",
    "\"\"\"ğŸ’¡ En este caso definimos las columnas al tipo Object(permite datos de diversos tipos)\n",
    "   para poder manejar el error de inconsistencia de datos en las columnas.\"\"\"\n",
    "# df_diez.head()\n",
    "# df_diez_cast = df_diez.with_columns(\n",
    "#     pl.col(\"ID\").map_elements(lambda x:int(x),return_dtype=pl.Int64).alias(\"ID\"),\n",
    "#     pl.col(\"Venta\").map_elements(lambda x:float(x),return_dtype=pl.Float64).alias(\"Venta\")\n",
    "# )\n",
    "# df_diez_cast.head()\n",
    "\"\"\"\n",
    "ğŸ’¡ Para esta soluciÃ³n, Polars indica que map_elements es ineficiente al castear\n",
    "    estos datos debido a que evalua fila a fila la funciÃ³n. Sin embargo, son estas\n",
    "    casuÃ­sticas que nos permiten optar por estas soluciones.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "11. IdentificaciÃ³n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogrÃ¡ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "âœï¸ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedroo\"]\n",
    "}\n",
    "import difflib\n",
    "nombres_estandarizados = [i.replace(' ','') for i in diccionario_once[\"cliente\"]]\n",
    "# nombres_estandarizados\n",
    "sospechosos_duplicados = {}\n",
    "for nombre in nombres_estandarizados:\n",
    "    sospechoso = difflib.get_close_matches(nombre,nombres_estandarizados,n=3,cutoff=0.8)\n",
    "    sospechosos_duplicados[nombre] = sospechoso\n",
    "sospechosos_duplicados \n",
    "\n",
    "\"\"\"\n",
    "12. ValidaciÃ³n y limpieza de rangos vÃ¡lidos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Valida que la columna \"age\" estÃ© en un rango lÃ³gico (0 a 100 aÃ±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "âœï¸ Resultado esperado: columna age sin valores invÃ¡lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce.shape[0] ## â¡ï¸ Cantidad de datos iniciales: 891\n",
    "df_doce_clean = df_doce.filter(\n",
    "    ((pl.col(\"age\")>0) & (pl.col(\"age\")<100))\n",
    ")\n",
    "# df_doce_clean.head()\n",
    "df_doce_clean.shape[0] ## â¡ï¸ Cantidad de datos filtrados: 714\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n de categorÃ­as inconsistentes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta y unifica las categorÃ­as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcciÃ³n de errores).\n",
    "âœï¸ Resultado esperado: Dataset con categorÃ­as Ãºnicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "import difflib\n",
    "df_trece = pl.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "ciudades_estandarizadas = [\"Lima\"] # Lista de ciudades estandarizadas\n",
    "def estandarizar_ciudad(valor):\n",
    "    similar = difflib.get_close_matches(valor,ciudades_estandarizadas,n=1,cutoff=0.6)\n",
    "    return similar[0] if similar else valor\n",
    "df_trece = df_trece.with_columns(\n",
    "    pl.col(\"Ciudad\").str.to_titlecase().map_batches(estandarizar_ciudad).alias(\"Ciudad\")\n",
    ")\n",
    "df_trece.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787fb12",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING (INGENIERÃA DE CARACTERÃSTICAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337f29c",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba96807",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4115cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>longitud_comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excelente servicio</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muy caro</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aceptable</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           comentario  longitud_comentario\n",
       "0  Excelente servicio                   18\n",
       "1            Muy caro                    8\n",
       "2           Aceptable                    9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "1. Variables dummies simples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte la columna sex en variables dummies\n",
    "âœï¸ Resultado esperado: Dos columnas adicionales (sex_male, sex_female) con valores binarios 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = sns.load_dataset(\"titanic\")\n",
    "# df_uno.head()\n",
    "df_dummies_sex = pd.get_dummies(data=df_uno[\"sex\"],columns=[\"sex\"])\n",
    "df_uno_final = pd.concat([df_uno,df_dummies_sex],axis=1)\n",
    "df_uno_final\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"OneHot-Encoding\"\n",
    "##    la cuÃ¡l permite transformar datos en valores Ã³ptimos para modelos de Machine Learning (ML).\n",
    "\n",
    "\"\"\"\n",
    "2. Binning por intervalos fijos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa la columna total_bill en 3 intervalos: bajo(<10), medio(>=10 y <20), alto(>=20).\n",
    "âœï¸ Resultado esperado: Nueva columna total_bill_bin con categorÃ­as: \"Bajo\", \"Medio\", \"Alto\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = sns.load_dataset(\"tips\")\n",
    "# df_dos.head()\n",
    "condiciones = [\n",
    "    (df_dos[\"total_bill\"]<10),\n",
    "    (df_dos[\"total_bill\"]>=10) & (df_dos[\"total_bill\"]<20),\n",
    "    (df_dos[\"total_bill\"]>20)\n",
    "]\n",
    "valores = np.array([\"Bajo\",\"Medio\",\"Alto\"],dtype=object)\n",
    "df_dos[\"total_bill_bin\"] = np.select(condlist=condiciones,choicelist=valores,default=\"F\")\n",
    "df_dos.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. NormalizaciÃ³n min-max\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Normaliza la columna ventas entre 0 y 1 usando min-max.\n",
    "âœï¸ Resultado esperado: Nueva columna ventas_norm con valores escalados entre 0 y 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_tres = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"ventas\":[100,300,500]\n",
    "}\n",
    "df_tres = pd.DataFrame(diccionario_tres)\n",
    "# df_tres.head()\n",
    "df_tres[\"ventas_normalizada\"] = ((df_tres[\"ventas\"]-df_tres[\"ventas\"].min())/(df_tres[\"ventas\"].max()-df_tres[\"ventas\"].min()))\n",
    "df_tres.head()\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"NormalizaciÃ³n Min-Max\"\n",
    "##    la cuÃ¡l permite establecer en un rango de 0 y 1 valores que permitan a modelos de ML\n",
    "##    aprener los patrones de los datos, pero con variables en escalas comparables mejorando su rendimiento.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ExtracciÃ³n de aÃ±o y mes de fechas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Enunciado: Extrae el aÃ±o y mes de la columna fecha.\n",
    "âœï¸ Resultado esperado: Dos nuevas columnas: aÃ±o y mes.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12\",\"2022-01-20\",\"2023-07-15\"]\n",
    "}\n",
    "df_cuatro = pd.DataFrame(data=diccionario_cuatro)\n",
    "# df_cuatro.head()\n",
    "df_cuatro[\"fecha\"] = pd.to_datetime(df_cuatro[\"fecha\"]) ## â¬…ï¸ Convertir la columna fecha (str) a tipo datetime\n",
    "df_cuatro[\"aÃ±o\"] = df_cuatro[\"fecha\"].dt.year ## â¬…ï¸ Extraer el AÃ±o\n",
    "df_cuatro[\"mes\"] = df_cuatro[\"fecha\"].dt.month ## â¬…ï¸ Extraer el Mes\n",
    "df_cuatro.head()\n",
    "\n",
    "\"\"\"\n",
    "5. Longitud de cadenas de texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con la longitud de caracteres de cada comentario.\n",
    "âœï¸ Resultado esperado: Columna longitud con nÃºmero de caracteres por fila.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"comentario\":[\"Excelente servicio\",\"Muy caro\",\"Aceptable\"]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "df_cinco[\"longitud_comentario\"] = df_cinco[\"comentario\"].str.len()\n",
    "df_cinco.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be1caa",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10032fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>longitud_caracter</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Excelente servicio&quot;</td><td>18</td></tr><tr><td>&quot;Muy caro&quot;</td><td>8</td></tr><tr><td>&quot;Aceptable&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario         â”† longitud_caracter â”‚\n",
       "â”‚ ---                â”† ---               â”‚\n",
       "â”‚ str                â”† u32               â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Excelente servicio â”† 18                â”‚\n",
       "â”‚ Muy caro           â”† 8                 â”‚\n",
       "â”‚ Aceptable          â”† 9                 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "1. Variables dummies simples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte la columna sex en variables dummies\n",
    "âœï¸ Resultado esperado: Dos columnas adicionales (sex_male, sex_female) con valores binarios 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_uno.head()\n",
    "\n",
    "df_sex_dummies = df_uno.to_dummies(columns=[\"sex\"])\n",
    "df_uno_final = pl.concat([df_uno,df_sex_dummies[[\"sex_male\",\"sex_female\"]]],how=\"horizontal\")\n",
    "df_uno_final\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"OneHot-Encoding\"\n",
    "##    la cuÃ¡l permite transformar datos en valores Ã³ptimos para modelos de Machine Learning (ML).\n",
    "\n",
    "\"\"\"\n",
    "2. Binning por intervalos fijos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa la columna total_bill en 3 intervalos: bajo(<10), medio(>=10 y <20), alto(>=20).\n",
    "âœï¸ Resultado esperado: Nueva columna total_bill_bin con categorÃ­as: \"Bajo\", \"Medio\", \"Alto\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_dos.head()\n",
    "df_dos = df_dos.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"total_bill\")<10\n",
    "    ).then(\n",
    "        pl.lit(\"Bajo\")\n",
    "    ).when(\n",
    "        (pl.col(\"total_bill\")>=10) & (pl.col(\"total_bill\")<20)\n",
    "    ).then(pl.lit(\"Medio\"))\n",
    "    .otherwise(pl.lit(\"Alto\"))\n",
    ")\n",
    "df_dos.head()\n",
    "\n",
    "\"\"\"\n",
    "3. NormalizaciÃ³n min-max\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Normaliza la columna ventas entre 0 y 1 usando min-max.\n",
    "âœï¸ Resultado esperado: Nueva columna ventas_norm con valores escalados entre 0 y 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_tres = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"ventas\":[100,300,500]\n",
    "}\n",
    "df_tres = pl.DataFrame(diccionario_tres)\n",
    "# df_tres.head()\n",
    "df_tres = df_tres.with_columns(\n",
    "    ((pl.col(\"ventas\")-pl.col(\"ventas\").min())/(pl.col(\"ventas\").max()-pl.col(\"ventas\").min())).alias(\"ventas_normalizada\")\n",
    ")\n",
    "df_tres.head()\n",
    "\n",
    "# ## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"NormalizaciÃ³n Min-Max\"\n",
    "# ##    la cuÃ¡l permite establecer en un rango de 0 y 1 valores que permitan a modelos de ML\n",
    "# ##    aprener los patrones de los datos, pero con variables en escalas comparables mejorando su rendimiento.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ExtracciÃ³n de aÃ±o y mes de fechas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Enunciado: Extrae el aÃ±o y mes de la columna fecha.\n",
    "âœï¸ Resultado esperado: Dos nuevas columnas: aÃ±o y mes.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12\",\"2022-01-20\",\"2023-07-15\"]\n",
    "}\n",
    "df_cuatro = pl.DataFrame(diccionario_cuatro)\n",
    "# df_cuatro.head()\n",
    "df_cuatro = df_cuatro.with_columns(\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).dt.year().alias(\"AÃ±o\"),\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).dt.month().alias(\"Mes\")\n",
    ")\n",
    "df_cuatro.head()\n",
    "\n",
    "\"\"\"\n",
    "5. Longitud de cadenas de texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con la longitud de caracteres de cada comentario.\n",
    "âœï¸ Resultado esperado: Columna longitud con nÃºmero de caracteres por fila.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"comentario\":[\"Excelente servicio\",\"Muy caro\",\"Aceptable\"]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "df_cinco = df_cinco.with_columns(\n",
    "    pl.col(\"comentario\").str.len_chars().alias(\"longitud_caracter\")\n",
    ")\n",
    "df_cinco.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2712f8b",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2adf4da",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce7ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>num_palabras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me gusta el servicio</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precio alto pero bueno</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No lo recomiendo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               comentario  num_palabras\n",
       "0    Me gusta el servicio             4\n",
       "1  Precio alto pero bueno             4\n",
       "2        No lo recomiendo             3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "6. Variables dummies mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte embarked en variables dummies.\n",
    "âœï¸ Resultado esperado: Nuevas columnas (embarked_C, embarked_Q, embarked_S) con valores 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = sns.load_dataset(\"titanic\")\n",
    "# df_seis.head()\n",
    "df_seis_embarked = pd.get_dummies(data=df_seis[\"embarked\"],prefix=\"embarked\")\n",
    "# df_seis_embarked.head()\n",
    "df_seis_final = pd.concat([df_seis,df_seis_embarked],axis=1)\n",
    "df_seis_final.head()\n",
    "\n",
    "\"\"\"\n",
    "7. Binning por cuantiles\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Divide la columna price en 4 categorÃ­as segÃºn sus cuartiles.\n",
    "âœï¸ Resultado esperado: Nueva columna price_bin con categorÃ­as Q1, Q2, Q3, Q4.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = sns.load_dataset(\"diamonds\")\n",
    "# df_siete.head()\n",
    "df_siete[\"price_bin\"] = pd.qcut(df_siete[\"price\"],q=4,labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"])\n",
    "df_siete.head()\n",
    "## ğŸ’¡ La divisiÃ³n de una columna en quantiles permite mantener la relaciÃ³n entre\n",
    "##    los datos originales y su clasificaciÃ³n por quantiles.\n",
    "\n",
    "\"\"\"\n",
    "8. EstandarizaciÃ³n (Z-score)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el Z-score de las notas.\n",
    "âœï¸ Resultado esperado: Nueva columna notas_zscore con valores centrados en media 0 y desviaciÃ³n estÃ¡ndar 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "lista_ocho = [\n",
    "    12, 14, 13, 15, 16, 11, 14, 13, 15, 12,  # Notas normales\n",
    "    14, 16, 13, 15, 14, 12, 13, 16, 15, 14,  # MÃ¡s notas normales\n",
    "    3,   # Nota muy baja (Outlier)\n",
    "    19   # Nota muy lta (Outlier)\n",
    "]\n",
    "\n",
    "df_ocho = pd.DataFrame({\n",
    "    \"Estudiante\": [f\"Estudiante {i+1}\" for i in range(len(lista_ocho))],\n",
    "    \"Nota\":lista_ocho\n",
    "})\n",
    "# df_ocho.head()\n",
    "\n",
    "## âœ… Calcular Z-SCORE\n",
    "##---- Importamos: from scipy import stats\n",
    "from scipy import stats\n",
    "\n",
    "df_ocho[\"notas_zscore\"] = stats.zscore(df_ocho[\"Nota\"])\n",
    "df_ocho.head()\n",
    "## ğŸ’¡ La estandarizaciÃ³n con z-score es una transformaciÃ³n estÃ¡ndar \n",
    "##    en ciencia de datos que \"normaliza\" los datos sin cambiar su distribuciÃ³n, solo su escala.\n",
    "\n",
    "\"\"\"\n",
    "9. DÃ­a de la semana desde fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae el dÃ­a de la semana de cada fecha.\n",
    "âœï¸ Resultado esperado: Columna dia_semana con valores tipo: \"Viernes\", \"SÃ¡bado\", \"Domingo\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2023-05-12\",\"2023-05-13\",\"2023-05-14\"]\n",
    "}\n",
    "df_nueve = pd.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "df_nueve[\"fecha\"] = pd.to_datetime(df_nueve[\"fecha\"])\n",
    "# df_nueve.head()\n",
    "df_nueve[\"dia_semana\"] = df_nueve[\"fecha\"].dt.day_name(locale=\"\")\n",
    "df_nueve.head()\n",
    "\n",
    "\"\"\"\n",
    "10. Conteo de palabras en texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras por comentario.\n",
    "âœï¸ Resultado esperado: Columna num_palabras con valores 4, 4, 3.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_diez = {\n",
    "    \"comentario\":[\"Me gusta el servicio\",\"Precio alto pero bueno\",\"No lo recomiendo\"]\n",
    "}\n",
    "\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez[\"num_palabras\"] = df_diez[\"comentario\"].apply(lambda x:len(x.split(\" \")))\n",
    "df_diez.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518f057",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a65b845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>s</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Me gusta el servicio&quot;</td><td>4</td></tr><tr><td>&quot;Precio alto pero bueno&quot;</td><td>4</td></tr><tr><td>&quot;No lo recomiendo&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario             â”† s   â”‚\n",
       "â”‚ ---                    â”† --- â”‚\n",
       "â”‚ str                    â”† i64 â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
       "â”‚ Me gusta el servicio   â”† 4   â”‚\n",
       "â”‚ Precio alto pero bueno â”† 4   â”‚\n",
       "â”‚ No lo recomiendo       â”† 3   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "6. Variables dummies mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte embarked en variables dummies.\n",
    "âœï¸ Resultado esperado: Nuevas columnas (embarked_C, embarked_Q, embarked_S) con valores 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_seis.head()\n",
    "df_seis_dummies = df_seis.select(pl.col(\"embarked\").drop_nulls()).to_dummies(columns=[\"embarked\"])\n",
    "# df_seis_dummies.head()\n",
    "df_seis_final = pl.concat([df_seis,df_seis_dummies],how=\"horizontal\")\n",
    "df_seis_final.head()\n",
    "\n",
    "\"\"\"\n",
    "7. Binning por cuantiles\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Divide la columna price en 4 categorÃ­as segÃºn sus cuartiles.\n",
    "âœï¸ Resultado esperado: Nueva columna price_bin con categorÃ­as Q1, Q2, Q3, Q4.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_siete.head()\n",
    "df_siete = df_siete.with_columns(\n",
    "    pl.col(\"price\").qcut(quantiles=4,labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"]).alias(\"price_bin\")\n",
    ")\n",
    "df_siete.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. EstandarizaciÃ³n (Z-score)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el Z-score de las notas.\n",
    "âœï¸ Resultado esperado: Nueva columna notas_zscore con valores centrados en media 0 y desviaciÃ³n estÃ¡ndar 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "lista_ocho = [\n",
    "    12, 14, 13, 15, 16, 11, 14, 13, 15, 12,  # Notas normales\n",
    "    14, 16, 13, 15, 14, 12, 13, 16, 15, 14,  # MÃ¡s notas normales\n",
    "    3,   # Nota muy baja (Outlier)\n",
    "    19   # Nota muy lta (Outlier)\n",
    "]\n",
    "\n",
    "df_ocho = pl.DataFrame({\n",
    "    \"Estudiante\": [f\"Estudiante {i+1}\" for i in range(len(lista_ocho))],\n",
    "    \"Nota\":lista_ocho\n",
    "})\n",
    "# df_ocho.head()\n",
    "\n",
    "## âœ… Calcular Z-SCORE\n",
    "##---- Importamos: from scipy import stats\n",
    "from scipy import stats\n",
    "### --- Extraemos la columna como serie Numpy\n",
    "nota = df_ocho[\"Nota\"].to_numpy()\n",
    "### --- Calculamos el z-score.\n",
    "serie_nota_zscore = stats.zscore(nota)\n",
    "df_ocho = df_ocho.with_columns(\n",
    "    pl.Series(\"nota_zscore\",serie_nota_zscore) ## Agregamos la serie como columna de Polars\n",
    ")\n",
    "df_ocho.head()\n",
    "\n",
    "\"\"\"\n",
    "9. DÃ­a de la semana desde fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae el dÃ­a de la semana de cada fecha.\n",
    "âœï¸ Resultado esperado: Columna dia_semana con valores tipo: \"Viernes\", \"SÃ¡bado\", \"Domingo\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2023-05-12\",\"2023-05-13\",\"2023-05-14\"]\n",
    "}\n",
    "df_nueve = pl.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).alias(\"fecha\")\n",
    ")\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.col(\"fecha\").dt.to_string(format=\"%A\").alias(\"nombre_dia\")\n",
    ")\n",
    "df_nueve.head()\n",
    "## ğŸ’¡ Es importante conocer los diversos formatos de fechas.\n",
    "\n",
    "\"\"\"\n",
    "10. Conteo de palabras en texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras por comentario.\n",
    "âœï¸ Resultado esperado: Columna num_palabras con valores 4, 4, 3.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_diez = {\n",
    "    \"comentario\":[\"Me gusta el servicio\",\"Precio alto pero bueno\",\"No lo recomiendo\"]\n",
    "}\n",
    "\n",
    "df_diez = pl.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez = df_diez.with_columns(\n",
    "    pl.col(\"comentario\").map_elements(lambda x:len(x.split(\" \")),return_dtype=pl.Int64).alias(\"s\")\n",
    ")\n",
    "df_diez.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026ded4",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94a393",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "726b1c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>palabras_unicas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muy buen buen servicio</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Servicio aceptable aceptable</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No me gustÃ³ el servicio</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     comentario  palabras_unicas\n",
       "0        Muy buen buen servicio                3\n",
       "1  Servicio aceptable aceptable                2\n",
       "2       No me gustÃ³ el servicio                5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "11. Variables categÃ³ricas cruzadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Crea una nueva variable categÃ³rica que combine pclass y sex (ejemplo: \"1_female\", \"3_male\").\n",
    "âœï¸ Resultado esperado: Columna clase_sexo con las combinaciones Ãºnicas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = sns.load_dataset(\"titanic\")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Creamos la variable que permita las combinaciones de datos en ambas columnas.\n",
    "df_once[\"clase_sexo\"] = df_once[\"pclass\"].astype(str)+'_'+df_once[\"sex\"]\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Categorizamos las variables de esa columna.\n",
    "dummies_pclass_sex = pd.get_dummies(data=df_once[[\"clase_sexo\"]],prefix=\"\")\n",
    "# dummies_pclass_sex\n",
    "\n",
    "## -- âœ… Unificamos al dataframe original\n",
    "df_once_final = pd.concat([df_once,dummies_pclass_sex],axis=\"columns\")\n",
    "df_once_final.head()\n",
    "\n",
    "\"\"\"\n",
    "12. Binning desigual basado en reglas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Clasifica tip en categorÃ­as: \"Bajo\" (<2), \"Medio\" (2 - 5), \"Alto\" (>5).\n",
    "âœï¸ Resultado esperado: Nueva columna tip_categoria con esas 3 categorÃ­as.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = sns.load_dataset(\"tips\")\n",
    "# df_doce.head()\n",
    "condiciones_valores = [\n",
    "    (df_doce[\"tip\"]<2),\n",
    "    (df_doce[\"tip\"]>=2) & (df_doce[\"tip\"]<=5),\n",
    "    (df_doce[\"tip\"]>5)\n",
    "]\n",
    "values = np.array([\"Bajo\",\"Medio\",\"Alto\"],object)\n",
    "df_doce[\"tip_categoria\"] = np.select(condlist=condiciones_valores,choicelist=values,default=\"F\")\n",
    "df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n robusta\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica una normalizaciÃ³n robusta (usando mediana e IQR).\n",
    "âœï¸ Resultado esperado: Columna ventas_robust que reduzca la influencia de outliers.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\",\"D\"],\n",
    "    \"ventas\":[10,200,1000,10000]\n",
    "}\n",
    "df_trece = pd.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "\n",
    "##  âœ… Hallamos los Quartiles (columna ventas)\n",
    "q1_ventas = float(np.quantile(df_trece[\"ventas\"],0.25))\n",
    "# q1_ventas\n",
    "q3_ventas = float(np.quantile(df_trece[\"ventas\"],0.75))\n",
    "# q3_ventas\n",
    "\n",
    "## âœ… Hallamos IQR (Rango Interquartil - columnas ventas)\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "# iqr_ventas\n",
    "\n",
    "## âœ… Hallamos mediana (columna ventas)\n",
    "mediana_ventas = float(df_trece[\"ventas\"].median())\n",
    "# mediana_ventas\n",
    "\n",
    "## âœ… Calculamos NormalizaciÃ³n robusta\n",
    "df_trece[\"ventas_robust\"] = ((df_trece[\"ventas\"] - mediana_ventas) / iqr_ventas).round(2)\n",
    "df_trece.head()\n",
    "\n",
    "## ğŸ’¡ La normalizaciÃ³n robusta es una tÃ©cnica de preprocesamiento de datos\n",
    "##    que escala las caracterÃ­sticas utilizando estadÃ­sticas que son menos\n",
    "##    sensibles a valores atÃ­picos (outliers) que la normalizaciÃ³n estÃ¡ndar.\n",
    "\n",
    "\"\"\"\n",
    "14. ExtracciÃ³n de partes avanzadas de fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae la hora, el minuto y el nombre del dÃ­a de la semana.\n",
    "âœï¸ Resultado esperado: Tres nuevas columnas: hora, minuto, dia_semana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12 14:35:00\",\"2021-05-12 20:10:00\",\"2021-05-13 08:45:00\"]\n",
    "}\n",
    "\n",
    "df_catorce = pd.DataFrame(diccionario_catorce)\n",
    "# df_catorce.head()\n",
    "df_catorce[\"fecha\"] = pd.to_datetime(df_catorce[\"fecha\"])\n",
    "# df_catorce.head()\n",
    "df_catorce[\"hora\"] = df_catorce[\"fecha\"].dt.hour\n",
    "df_catorce[\"minuto\"] = df_catorce[\"fecha\"].dt.minute\n",
    "df_catorce[\"dia_semana\"] = df_catorce[\"fecha\"].dt.day_name(locale=\"\")\n",
    "df_catorce.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "15. ExtracciÃ³n de features de texto (tokens Ãºnicos)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras Ãºnicas en cada comentario.\n",
    "âœï¸ Resultado esperado: Columna palabras_unicas con valores (3, 2, 4).\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_quince = {\n",
    "    \"comentario\":[\"Muy buen buen servicio\",\"Servicio aceptable aceptable\",\"No me gustÃ³ el servicio\"]\n",
    "}\n",
    "df_quince = pd.DataFrame(diccionario_quince)\n",
    "df_quince[\"palabras_unicas\"] = df_quince[\"comentario\"].apply(lambda x:len(set(x.split(\" \"))))\n",
    "df_quince.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873079e7",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ad4d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>palabras_unicas</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Muy buen buen servicio&quot;</td><td>3</td></tr><tr><td>&quot;Servicio aceptable aceptable&quot;</td><td>2</td></tr><tr><td>&quot;No me gustÃ³ el servicio&quot;</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario                   â”† palabras_unicas â”‚\n",
       "â”‚ ---                          â”† ---             â”‚\n",
       "â”‚ str                          â”† i64             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Muy buen buen servicio       â”† 3               â”‚\n",
       "â”‚ Servicio aceptable aceptable â”† 2               â”‚\n",
       "â”‚ No me gustÃ³ el servicio      â”† 5               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "11. Variables categÃ³ricas cruzadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Crea una nueva variable categÃ³rica que combine pclass y sex (ejemplo: \"1_female\", \"3_male\").\n",
    "âœï¸ Resultado esperado: Columna clase_sexo con las combinaciones Ãºnicas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Creamos la variable que permita las combinaciones de datos en ambas columnas.\n",
    "df_once = df_once.with_columns(\n",
    "    (pl.col(\"pclass\").cast(pl.String)+\"-\"+pl.col(\"sex\")).alias(\"clase_sexo\")\n",
    ")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Categorizamos las variables de esa columna.\n",
    "dummies_pclass_sex = df_once.select(pl.col(\"clase_sexo\")).to_dummies(columns=[\"clase_sexo\"])\n",
    "dummies_pclass_sex\n",
    "\n",
    "## -- âœ… Unificamos al dataframe original\n",
    "df_once_final = pl.concat([df_once,dummies_pclass_sex],how=\"horizontal\")\n",
    "df_once_final.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. Binning desigual basado en reglas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Clasifica tip en categorÃ­as: \"Bajo\" (<2), \"Medio\" (2 - 5), \"Alto\" (>5).\n",
    "âœï¸ Resultado esperado: Nueva columna tip_categoria con esas 3 categorÃ­as.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce = df_doce.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"tip\")<2\n",
    "    ).then(pl.lit(\"Bajo\"))\n",
    "    .when(\n",
    "        (pl.col(\"tip\")>=2) & (pl.col(\"tip\")<=5)\n",
    "    ).then(pl.lit(\"Medio\"))\n",
    "    .otherwise(pl.lit(\"Alto\"))\n",
    ")\n",
    "df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n robusta\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica una normalizaciÃ³n robusta (usando mediana e IQR).\n",
    "âœï¸ Resultado esperado: Columna ventas_robust que reduzca la influencia de outliers.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\",\"D\"],\n",
    "    \"ventas\":[10,200,1000,10000]\n",
    "}\n",
    "df_trece = pl.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "\n",
    "##  âœ… Hallamos los Quartiles (columna ventas)\n",
    "q1_ventas = df_trece[\"ventas\"].quantile(0.25)\n",
    "# q1_ventas\n",
    "q3_ventas = df_trece[\"ventas\"].quantile(0.75)\n",
    "# q3_ventas\n",
    "\n",
    "## âœ… Hallamos IQR (Rango Interquartil - columnas ventas)\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "# iqr_ventas\n",
    "\n",
    "## âœ… Hallamos mediana (columna ventas)\n",
    "mediana_ventas = float(df_trece[\"ventas\"].median())\n",
    "# mediana_ventas\n",
    "\n",
    "## âœ… Calculamos NormalizaciÃ³n robusta\n",
    "df_trece = df_trece.with_columns(\n",
    "    ((pl.col(\"ventas\") - mediana_ventas)/iqr_ventas).round(2).alias(\"ventas_robust\")\n",
    ")\n",
    "df_trece.head()\n",
    "\n",
    "## ğŸ’¡ La normalizaciÃ³n robusta es una tÃ©cnica de preprocesamiento de datos\n",
    "##    que escala las caracterÃ­sticas utilizando estadÃ­sticas que son menos\n",
    "##    sensibles a valores atÃ­picos (outliers) que la normalizaciÃ³n estÃ¡ndar.\n",
    "\n",
    "\"\"\"\n",
    "14. ExtracciÃ³n de partes avanzadas de fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae la hora, el minuto y el nombre del dÃ­a de la semana.\n",
    "âœï¸ Resultado esperado: Tres nuevas columnas: hora, minuto, dia_semana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12 14:35:00\",\"2021-05-12 20:10:00\",\"2021-05-13 08:45:00\"]\n",
    "}\n",
    "\n",
    "df_catorce = pl.DataFrame(diccionario_catorce)\n",
    "# df_catorce.head()\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    pl.col(\"fecha\").str.to_datetime().alias(\"fecha\")\n",
    ")\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    pl.col(\"fecha\").dt.hour().alias(\"hora\"),\n",
    "    pl.col(\"fecha\").dt.minute().alias(\"minuto\"),\n",
    "    pl.col(\"fecha\").dt.strftime(format=\"%A\").alias(\"dia_semana\")\n",
    ")\n",
    "df_catorce.head()\n",
    "\n",
    "\"\"\"\n",
    "15. ExtracciÃ³n de features de texto (tokens Ãºnicos)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras Ãºnicas en cada comentario.\n",
    "âœï¸ Resultado esperado: Columna palabras_unicas con valores (3, 2, 4).\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_quince = {\n",
    "    \"comentario\":[\"Muy buen buen servicio\",\"Servicio aceptable aceptable\",\"No me gustÃ³ el servicio\"]\n",
    "}\n",
    "df_quince = pl.DataFrame(diccionario_quince)\n",
    "df_quince = df_quince.with_columns(\n",
    "    pl.col(\"comentario\").map_elements(lambda x:len(set(x.split(\" \"))),return_dtype=pl.Int64).alias(\"palabras_unicas\")\n",
    ")\n",
    "df_quince.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a381c3",
   "metadata": {},
   "source": [
    "#### AGREGACIONES Y AGRUPACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094bfbd0",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d7f98",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c60e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.29"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "1. Suma por categorÃ­a\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Calcula el total de total_bill por cada valor de day.\n",
    "âœï¸ Resultado esperado: Tabla con dÃ­as (Thur, Fri, etc.) y la suma total de total_bill para cada uno.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = sns.load_dataset(\"tips\")\n",
    "# df_uno.head()\n",
    "df_uno_agrupado = df_uno.groupby(\"day\",as_index=False,observed=True)[\"total_bill\"].sum().reset_index(drop=True,level=0)\n",
    "df_uno_agrupado.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Media por dos columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: ObtÃ©n el promedio de age agrupado por sex y pclass.\n",
    "âœï¸ Resultado esperado: Tabla con Ã­ndice jerÃ¡rquico (sex/pclass) y columna con la edad promedio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = sns.load_dataset(\"titanic\")\n",
    "# df_dos.head()\n",
    "df_dos_agrupado = df_dos.groupby([\"sex\",\"pclass\"],as_index=True,observed=True)[\"age\"].mean().round(2)\n",
    "df_dos_agrupado.head()\n",
    "\n",
    "\"\"\"\n",
    "3. Conteo de pasajeros por puerto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Cuenta cuÃ¡ntos pasajeros embarcaron en cada embarked.\n",
    "âœï¸ Resultado esperado: Serie con embarked como Ã­ndice y conteo de pasajeros.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = sns.load_dataset(\"titanic\")\n",
    "# df_tres.head()\n",
    "df_tres_agrupado = df_tres.groupby(\"embarked\",as_index=True,observed=True).size()\n",
    "df_tres_agrupado.head()\n",
    "# print(type(df_tres_agrupado)) ## El tipo de estructura que devuelve es una \"Serie\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. Pivot table simple\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre el promedio de tip por day.\n",
    "âœï¸ Resultado esperado: Tabla con dÃ­as como filas y una columna con el promedio de propina.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = sns.load_dataset(\"tips\")\n",
    "# df_cuatro.head()\n",
    "df_cuatro_pivot = df_cuatro.pivot_table(index=\"day\",values=\"tip\",aggfunc=\"mean\",observed=True).round(2).rename({\"tip\":\"avg_tip\"},axis=1)\n",
    "df_cuatro_pivot.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. Promedio ponderado bÃ¡sico\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el precio promedio ponderado segÃºn las ventas.\n",
    "âœï¸ Resultado esperado: Valor Ãºnico representando el precio medio considerando el peso de las ventas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"precio\":[10,20,30],\n",
    "    \"ventas\":[100,50,20]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "df_cinco[\"precio_ventas\"] = df_cinco[\"precio\"]*df_cinco[\"ventas\"]\n",
    "# df_cinco.head()\n",
    "suma_precio_ventas = float(df_cinco[\"precio_ventas\"].sum())\n",
    "# suma_valor_peso\n",
    "suma_pesos_ventas = float(df_cinco[\"ventas\"].sum())\n",
    "# suma_pesos_ventas\n",
    "precio_ponderado = round((suma_precio_ventas/suma_pesos_ventas),2)\n",
    "precio_ponderado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf095904",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84edae8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.29"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "## âœ… En esta ocasiÃ³n usaremos los LazyFrames de Polars (en algunos ejercicios) !!!\n",
    "\n",
    "\"\"\"\n",
    "1. Suma por categorÃ­a\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Calcula el total de total_bill por cada valor de day.\n",
    "âœï¸ Resultado esperado: Tabla con dÃ­as (Thur, Fri, etc.) y la suma total de total_bill para cada uno.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = pl.scan_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_uno.collect()\n",
    "df_uno_agrupado = df_uno.group_by(\"day\").agg(\n",
    "    pl.col(\"total_bill\").sum().alias(\"total_bill_day\")\n",
    ")\n",
    "df_uno_agrupado.collect()\n",
    "\n",
    "\"\"\"\n",
    "2. Media por dos columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: ObtÃ©n el promedio de age agrupado por sex y pclass.\n",
    "âœï¸ Resultado esperado: Tabla con Ã­ndice jerÃ¡rquico (sex/pclass) y columna con la edad promedio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_dos.head()\n",
    "\n",
    "df_dos_agrupado = df_dos.group_by([\"sex\",\"pclass\"]).agg(\n",
    "    pl.col(\"age\").mean().round(2).alias(\"avg_age\")\n",
    ")\n",
    "df_dos_agrupado.head()\n",
    "\n",
    "\"\"\"\n",
    "3. Conteo de pasajeros por puerto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Cuenta cuÃ¡ntos pasajeros embarcaron en cada embarked.\n",
    "âœï¸ Resultado esperado: Serie con embarked como Ã­ndice y conteo de pasajeros.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = pl.scan_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_tres.collect()\n",
    "df_tres = df_tres.filter(pl.col(\"embarked\").is_not_null())\n",
    "df_tres_agrupado = df_tres.group_by(\"embarked\").agg(\n",
    "    pl.col(\"survived\").count().alias(\"total_pasajeros\")\n",
    ")\n",
    "df_tres_agrupado.collect()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. Pivot table simple\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre el promedio de tip por day.\n",
    "âœï¸ Resultado esperado: Tabla con dÃ­as como filas y una columna con el promedio de propina.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_cuatro.head()\n",
    "df_cuatro_result = df_cuatro.group_by(\"day\").agg(\n",
    "    pl.col(\"tip\").mean().round(2).alias(\"avg_tip\")\n",
    ")\n",
    "df_cuatro_result.head()\n",
    "\n",
    "\"\"\"\n",
    "5. Promedio ponderado bÃ¡sico\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el precio promedio ponderado segÃºn las ventas.\n",
    "âœï¸ Resultado esperado: Valor Ãºnico representando el precio medio considerando el peso de las ventas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"precio\":[10,20,30],\n",
    "    \"ventas\":[100,50,20]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "df_cinco.head()\n",
    "df_cinco = df_cinco.with_columns(\n",
    "    (pl.col(\"precio\")*pl.col(\"ventas\")).alias(\"precio_ventas\")\n",
    ")\n",
    "df_cinco.head()\n",
    "suma_precio_ventas = df_cinco.select(pl.col(\"precio_ventas\").sum()).item(0,0)\n",
    "# suma_precio_ventas\n",
    "suma_pesos_ventas = df_cinco.select(pl.col(\"ventas\").sum()).item(0,0)\n",
    "# suma_pesos_ventas\n",
    "precio_ponderado = round((suma_precio_ventas/suma_pesos_ventas),2)\n",
    "precio_ponderado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697a47b",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788eb74e",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a10ed48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Norte    11.2\n",
       "Sur      12.2\n",
       "dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "6. GroupBy con mÃºltiples agregaciones\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por day y calcula: promedio de total_bill y suma de tip en una sola operaciÃ³n.\n",
    "âœï¸ Resultado esperado: DataFrame con day como Ã­ndice y dos columnas: total_bill_mean y tip_sum.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = sns.load_dataset(\"tips\")\n",
    "# df_seis.head()\n",
    "df_seis_agrupado = df_seis.groupby(\"day\",as_index=True,observed=True).agg(\n",
    "    avg_total_bill = pd.NamedAgg(column=\"total_bill\",aggfunc=\"mean\"),\n",
    "    sum_tip = pd.NamedAgg(column=\"tip\",aggfunc=\"sum\")\n",
    ").round(2)\n",
    "df_seis_agrupado.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. AgregaciÃ³n personalizada con lambda\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por pclass y calcula el rango (mÃ¡x - mÃ­n) de fare usando una funciÃ³n lambda.\n",
    "âœï¸ Resultado esperado: Serie con pclass y el rango de tarifas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = sns.load_dataset(\"titanic\")\n",
    "# df_siete.head()\n",
    "df_siete_agrupado = df_siete.groupby(\"pclass\",as_index=False,observed=True)[\"fare\"].aggregate(lambda x:\n",
    "                                            (max(x)-min(x))\n",
    "                                            ).round(2).rename({\"fare\":\"rango_fare\"},axis=1)\n",
    "df_siete_agrupado.head()\n",
    "\n",
    "\"\"\"\n",
    "8. Multi-index avanzado\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por cut y color, obteniendo la mediana de price.\n",
    "âœï¸ Resultado esperado: DataFrame con Ã­ndice jerÃ¡rquico (cut/color) y columna price_median.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = sns.load_dataset(\"diamonds\")\n",
    "# df_ocho.head()\n",
    "df_ocho_agrupado = df_ocho.groupby([\"cut\",\"color\"],as_index=True,observed=True)[\"price\"].agg(\"median\")\n",
    "df_ocho_agrupado.head(20)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "9. Pivot table con agregaciÃ³n mÃºltiple\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre por day el promedio y la suma de total_bill.\n",
    "âœï¸ Resultado esperado: Tabla con day como filas y dos columnas: avg_total_bill y sum_total_bill.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_nueve = sns.load_dataset(\"tips\")\n",
    "# df_nueve.head()\n",
    "df_nueve_pivoteado = df_nueve.pivot_table(index=\"day\",values=\"total_bill\",aggfunc=[\"mean\",\"sum\"],observed=True).round(2)\n",
    "df_nueve_pivoteado.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "10. Weighted average por categorÃ­a\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Enunciado: Calcula el precio promedio ponderado por regiÃ³n, usando ventas como peso.\n",
    "âœï¸ Resultado esperado: Serie con region como Ã­ndice y el precio ponderado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"region\":[\"Norte\",\"Norte\",\"Sur\",\"Sur\"],\n",
    "    \"ventas\":[100,150,80,120],\n",
    "    \"precio\":[10,12,8,15]\n",
    "}\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez[\"ventas_precio\"] = (df_diez[\"ventas\"]*df_diez[\"precio\"])\n",
    "# df_diez.head()\n",
    "def precio_ponderado_region(df,regiones):\n",
    "    serie_region = pd.Series(data=[\"\",\"\"],index=regiones)\n",
    "    for i,region in enumerate(regiones):\n",
    "        df_region = df.query(f\"region=='{region}'\")        \n",
    "        suma_ventas_precio_region = float(df_region[\"ventas_precio\"].sum())\n",
    "        suma_pesos_ventas_region = float(df_region[\"ventas\"].sum())\n",
    "        precio_ponderado = round(suma_ventas_precio_region/suma_pesos_ventas_region,2)\n",
    "        serie_region.iloc[i]=precio_ponderado\n",
    "    return serie_region\n",
    "serie_ponderado_regiones = precio_ponderado_region(df_diez,df_diez[\"region\"].unique().tolist())\n",
    "serie_ponderado_regiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f11f6b",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a1a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>region</th><th>precio_ponderado</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Sur&quot;</td><td>12.2</td></tr><tr><td>&quot;Norte&quot;</td><td>11.2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ region â”† precio_ponderado â”‚\n",
       "â”‚ ---    â”† ---              â”‚\n",
       "â”‚ str    â”† f64              â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Sur    â”† 12.2             â”‚\n",
       "â”‚ Norte  â”† 11.2             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "6. GroupBy con mÃºltiples agregaciones\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por day y calcula: promedio de total_bill y suma de tip en una sola operaciÃ³n.\n",
    "âœï¸ Resultado esperado: DataFrame con day como Ã­ndice y dos columnas: total_bill_mean y tip_sum.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.scan_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_seis.collect()\n",
    "df_seis_agrupado = df_seis.group_by(\"day\").agg(\n",
    "    pl.col(\"total_bill\").mean().round(2).alias(\"avg_total_bill\"),\n",
    "    pl.col(\"tip\").sum().round(2).alias(\"sum_tip\")\n",
    ")\n",
    "df_seis_agrupado.collect()\n",
    "\n",
    "\"\"\"\n",
    "7. AgregaciÃ³n personalizada con lambda\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por pclass y calcula el rango (mÃ¡x - mÃ­n) de fare usando una funciÃ³n lambda.\n",
    "âœï¸ Resultado esperado: Serie con pclass y el rango de tarifas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = pl.scan_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_siete.collect()\n",
    "df_siete_agrupado = df_siete.group_by(\"pclass\").agg(\n",
    "    pl.col(\"fare\").map_elements(lambda x:max(x)-min(x),return_dtype=pl.Float64).round(2).alias(\"rango_max_min_fare\")\n",
    ")\n",
    "df_siete_agrupado.collect()\n",
    "\n",
    "\"\"\"\n",
    "8. Multi-index avanzado\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por cut y color, obteniendo la mediana de price.\n",
    "âœï¸ Resultado esperado: DataFrame con Ã­ndice jerÃ¡rquico (cut/color) y columna price_median.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = pl.scan_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_ocho.collect()\n",
    "df_ocho_agrupado = df_ocho.group_by([\"cut\",\"color\"]).agg(\n",
    "    pl.col(\"price\").median().round(2).alias(\"median_price\")\n",
    ")\n",
    "df_ocho_agrupado.collect()\n",
    "\n",
    "\"\"\"\n",
    "9. Pivot table con agregaciÃ³n mÃºltiple\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre por day el promedio y la suma de total_bill.\n",
    "âœï¸ Resultado esperado: Tabla con day como filas y dos columnas: avg_total_bill y sum_total_bill.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_nueve = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_nueve.head()\n",
    "df_nueve_pivoteado = df_nueve.group_by(\"day\").agg(\n",
    "    pl.col(\"total_bill\").mean().round(2).alias(\"avg_total_bill\"),\n",
    "    pl.col(\"total_bill\").sum().round(2).alias(\"sum_total_bill\")\n",
    ")\n",
    "df_nueve_pivoteado.head()\n",
    "\n",
    "\"\"\".\n",
    "10. Weighted average por categorÃ­a\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el precio promedio ponderado por regiÃ³n, usando ventas como peso.\n",
    "âœï¸ Resultado esperado: Serie con region como Ã­ndice y el precio ponderado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"region\":[\"Norte\",\"Norte\",\"Sur\",\"Sur\"],\n",
    "    \"ventas\":[100,150,80,120],\n",
    "    \"precio\":[10,12,8,15]\n",
    "}\n",
    "df_diez = pl.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez = df_diez.with_columns(\n",
    "    (pl.col(\"ventas\")*pl.col(\"precio\")).alias(\"ventas_precio\")\n",
    ")\n",
    "regiones = [i[0] for i in df_diez.select(pl.col(\"region\").unique()).iter_rows()]\n",
    "def precio_ponderado_region(df,regiones):\n",
    "    df_region = []\n",
    "    dicccionario = dict()\n",
    "    for region in regiones:\n",
    "        df2 = df.filter(pl.col(\"region\")==region)\n",
    "        df2.head()\n",
    "        suma_ventas_precio = df2.select(pl.col(\"ventas_precio\").sum()).item()\n",
    "        suma_pesos_ventas = df2.select(pl.col(\"ventas\").sum()).item()\n",
    "        precio_ponderado = round((suma_ventas_precio/suma_pesos_ventas),2)\n",
    "        dicccionario[region] = precio_ponderado\n",
    "        df_temp = pl.DataFrame({\n",
    "            \"region\":region,\n",
    "            \"precio_ponderado\":precio_ponderado\n",
    "        })\n",
    "        df_region.append(df_temp)\n",
    "    if df_region:\n",
    "        return pl.concat(df_region)\n",
    "    else:\n",
    "        return pl.DataFrame({\"region\":[],\"precio_ponderado\":[]})\n",
    "df = precio_ponderado_region(df=df_diez,regiones=regiones)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37404ea2",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae7939",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6ead284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day\n",
       "Sat    50.81\n",
       "Sun    48.17\n",
       "Name: total_bill, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "11. GroupBy con mÃºltiples funciones y columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por pclass y sex, calculando:\n",
    "\n",
    "    . Promedio de age\n",
    "\n",
    "    . DesviaciÃ³n estÃ¡ndar de fare\n",
    "\n",
    "    . Conteo de pasajeros\n",
    "    \n",
    "âœï¸ Resultado esperado: DataFrame con multi-index (pclass, sex) y tres columnas con las mÃ©tricas.\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = sns.load_dataset(\"titanic\")\n",
    "# df_once.head()\n",
    "df_once_agrupado = df_once.groupby([\"pclass\",\"sex\"],observed=True,as_index=True).aggregate(\n",
    "    av_age = pd.NamedAgg(column=\"age\",aggfunc=\"mean\"),\n",
    "    std_fare = pd.NamedAgg(column=\"fare\",aggfunc=\"std\"),\n",
    "    count_passengers = pd.NamedAgg(column=\"survived\",aggfunc=\"count\")\n",
    ").round(2)\n",
    "df_once_agrupado.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. AgregaciÃ³n condicional con lambda\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Por cada day, calcula la suma de total_bill solo para registros donde size > 2, usando una funciÃ³n personalizada.    \n",
    "âœï¸ Resultado esperado: Serie con el total de total_bill por dÃ­a filtrado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = sns.load_dataset(\"tips\")\n",
    "# df_doce.head()\n",
    "\n",
    "def sum_total_bill(df):\n",
    "    df = df.query('size>2')\n",
    "    df_ag = df.groupby(\"day\",as_index=False,observed=True)[\"total_bill\"].sum()\n",
    "    return df_ag.set_index(\"day\")[\"total_bill\"] ## Retorna una serie, basÃ¡ndose en una columna e Ã­ndice\n",
    "\n",
    "df_doce_agrupado = sum_total_bill(df=df_doce)\n",
    "df_doce_agrupado.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. Pivot table dinÃ¡mica con columnas mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre para cada cut (filas) y color (columnas) el precio promedio.   \n",
    "âœï¸ Resultado esperado: Matriz con cut en filas, color en columnas y valores de price promedio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_trece = sns.load_dataset(\"diamonds\")\n",
    "# df_trece.head()\n",
    "df_trece_pivot = df_trece.pivot_table(index=\"cut\",columns=\"color\",values=\"price\",aggfunc=\"mean\",observed=True).round(2)\n",
    "df_trece_pivot.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "14. Promedio ponderado dentro de un groupby\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio ponderado de valor para cada categoria, usando peso como peso. \n",
    "âœï¸ Resultado esperado: Serie con categoria y su promedio ponderado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"categoria\":[\"A\",\"A\",\"B\",\"B\"],\n",
    "    \"peso\":[2,3,5,1],\n",
    "    \"valor\":[10,20,30,40]\n",
    "}\n",
    "df_catorce = pd.DataFrame(diccionario_catorce)\n",
    "df_catorce[\"peso_valor\"] = df_catorce[\"peso\"]*df_catorce[\"valor\"]\n",
    "# df_catorce.head()\n",
    "def valor_ponderado(df,categorias):\n",
    "    serie = pd.Series(name=\"precio_ponderado_categorias\")\n",
    "    for categoria in categorias:\n",
    "        df_categoria = df.query(f'categoria==\"{categoria}\"')\n",
    "        suma_peso_valor = df_categoria[\"peso_valor\"].sum()\n",
    "        suma_pesos = df_categoria[\"peso\"].sum()\n",
    "        valor_ponderado_categoria = round(suma_peso_valor/suma_pesos,2)\n",
    "        serie[categoria] = valor_ponderado_categoria\n",
    "    return serie\n",
    "df_catorce_final = valor_ponderado(df_catorce,df_catorce[\"categoria\"].unique())\n",
    "df_catorce_final.head()\n",
    "\n",
    "\"\"\"\n",
    "15. GroupBy con orden y top N\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Para cada day, encuentra el mÃ¡ximo de total_bill y muestra los 2 dÃ­as con mayor valor mÃ¡ximo.\n",
    "âœï¸ Resultado esperado: Serie ordenada con los 2 dÃ­as que tienen las cuentas mÃ¡s altas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_quince = sns.load_dataset(\"tips\")\n",
    "# df_quince.head()\n",
    "df_quince_agrupado = df_quince.groupby(\"day\",as_index=True,observed=True)[\"total_bill\"].max()\n",
    "df_quince_agrupado.sort_values(ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86d59c",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6b63240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>day</th><th>max_total_bill</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Sat&quot;</td><td>50.81</td></tr><tr><td>&quot;Sun&quot;</td><td>48.17</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ day â”† max_total_bill â”‚\n",
       "â”‚ --- â”† ---            â”‚\n",
       "â”‚ str â”† f64            â”‚\n",
       "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Sat â”† 50.81          â”‚\n",
       "â”‚ Sun â”† 48.17          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "11. GroupBy con mÃºltiples funciones y columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Agrupa por pclass y sex, calculando:\n",
    "\n",
    "    . Promedio de age\n",
    "\n",
    "    . DesviaciÃ³n estÃ¡ndar de fare\n",
    "\n",
    "    . Conteo de pasajeros\n",
    "    \n",
    "âœï¸ Resultado esperado: DataFrame con multi-index (pclass, sex) y tres columnas con las mÃ©tricas.\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = pl.scan_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_once.collect()\n",
    "df_once_agrupado = df_once.group_by([\"pclass\",\"sex\"]).agg(\n",
    "    pl.col(\"age\").mean().round(2).alias(\"avg_age\"),\n",
    "    pl.col(\"fare\").std().round(2).alias(\"std_fare\"),\n",
    "    pl.col(\"survived\").len().alias(\"total_pasajeros\")\n",
    ")\n",
    "df_once_agrupado.collect()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. AgregaciÃ³n condicional con lambda\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Por cada day, calcula la suma de total_bill solo para registros donde size > 2, usando una funciÃ³n personalizada.    \n",
    "âœï¸ Resultado esperado: Serie con el total de total_bill por dÃ­a filtrado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "def total_bill_calculo(df):\n",
    "    df = df.filter(\n",
    "        pl.col(\"size\")>2\n",
    "    )\n",
    "    df_agg = df.group_by(\"day\").agg(\n",
    "        pl.col(\"total_bill\").sum().alias(\"sum_total_bill\")\n",
    "    ) \n",
    "    return df_agg\n",
    "df_doce_agrupado = total_bill_calculo(df_doce)\n",
    "df_doce_agrupado.head()\n",
    "\n",
    "\"\"\"\n",
    "13. Pivot table dinÃ¡mica con columnas mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Crea una tabla pivote que muestre para cada cut (filas) y color (columnas) el precio promedio.   \n",
    "âœï¸ Resultado esperado: Matriz con cut en filas, color en columnas y valores de price promedio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_trece = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_trece.headt()\n",
    "df_trece_pivot = df_trece.pivot(index=\"cut\",on=\"color\",values=\"price\",aggregate_function=pl.element().mean().round(2))\n",
    "df_trece_pivot.head()\n",
    "\n",
    "\"\"\"\n",
    "14. Promedio ponderado dentro de un groupby\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio ponderado de valor para cada categoria, usando peso como peso. \n",
    "âœï¸ Resultado esperado: Serie con categoria y su promedio ponderado.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"categoria\":[\"A\",\"A\",\"B\",\"B\"],\n",
    "    \"peso\":[2,3,5,1],\n",
    "    \"valor\":[10,20,30,40]\n",
    "}\n",
    "df_catorce =  pl.DataFrame(diccionario_catorce)\n",
    "# df_catorce.head()\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    (pl.col(\"peso\")*pl.col(\"valor\")).alias(\"peso_ventas\")\n",
    ")\n",
    "# df_catorce.head()\n",
    "def peso_ponderado_categoria(df,categorias):\n",
    "    df_cat = []\n",
    "    for categoria in categorias:\n",
    "        df_categoria = df.filter(pl.col(\"categoria\")==categoria[0])\n",
    "        suma_peso_valor = df_categoria.select(pl.col(\"peso_ventas\").sum()).item()\n",
    "        suma_pesos = df_categoria.select(pl.col(\"peso\").sum()).item()\n",
    "        peso_ponderado = round((suma_peso_valor/suma_pesos),2)\n",
    "        df_temp = pl.DataFrame({\n",
    "            \"categoria\":categoria,\n",
    "            \"peso_ponderado\":peso_ponderado\n",
    "        })\n",
    "        df_cat.append(df_temp)\n",
    "    if df_cat:\n",
    "        return pl.concat(df_cat)\n",
    "    else:\n",
    "        return pl.DataFrame({\"categoria\":[],\"peso_ponderado\":[]})\n",
    "df = peso_ponderado_categoria(df_catorce,df_catorce.select(pl.col(\"categoria\").unique()).iter_rows())\n",
    "df.head()\n",
    "\n",
    "\"\"\"\n",
    "15. GroupBy con orden y top N\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Para cada day, encuentra el mÃ¡ximo de total_bill y muestra los 2 dÃ­as con mayor valor mÃ¡ximo.\n",
    "âœï¸ Resultado esperado: Serie ordenada con los 2 dÃ­as que tienen las cuentas mÃ¡s altas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_quince = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_quince.head()\n",
    "df_quince_agrupado = df_quince.group_by(\"day\").agg(\n",
    "    pl.col(\"total_bill\").max().alias(\"max_total_bill\")\n",
    ").sort(by=\"max_total_bill\",descending=True)\n",
    "df_quince_agrupado.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be9c17",
   "metadata": {},
   "source": [
    "#### WINDOW FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3099c4",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a9ba",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "229ed7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>passengers</th>\n",
       "      <th>percentil_movil_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949</td>\n",
       "      <td>Jan</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949</td>\n",
       "      <td>Feb</td>\n",
       "      <td>118</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>Mar</td>\n",
       "      <td>132</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949</td>\n",
       "      <td>Apr</td>\n",
       "      <td>129</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949</td>\n",
       "      <td>May</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year month  passengers  percentil_movil_75\n",
       "0  1949   Jan         112                 NaN\n",
       "1  1949   Feb         118                 NaN\n",
       "2  1949   Mar         132                 NaN\n",
       "3  1949   Apr         129                 NaN\n",
       "4  1949   May         121                 NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1. Promedio mÃ³vil de ventas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio mÃ³vil de 3 dÃ­as sobre la columna ventas.\n",
    "âœï¸ Resultado esperado: Nueva columna con el promedio de los Ãºltimos 3 dÃ­as (primeros 2 valores â†’ NaN).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_uno = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\"],\n",
    "    \"ventas\":[100,150,130,170,200]\n",
    "}\n",
    "df_uno = pd.DataFrame(diccionario_uno)\n",
    "df_uno[\"fecha\"] = pd.to_datetime(df_uno[\"fecha\"])\n",
    "# df_uno.head()\n",
    "\n",
    "df_uno[\"promedio_movil_3D\"] = df_uno[\"ventas\"].rolling(window=3).mean().round(2).reset_index(level=0,drop=True)\n",
    "df_uno.head(10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Suma acumulada (expanding sum)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: ObtÃ©n la suma acumulada de passengers por aÃ±o (year).\n",
    "âœï¸ Resultado esperado: Columna que acumule progresivamente los pasajeros en cada aÃ±o.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = sns.load_dataset(\"flights\")\n",
    "# df_dos.head()\n",
    "df_dos[\"suma_acumulada_anual\"] = df_dos.groupby(\"year\")[\"passengers\"].cumsum()\n",
    "df_dos.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Ranking de propinas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Asigna un ranking a cada registro de tip dentro de cada day, del mayor al menor.\n",
    "âœï¸ Resultado esperado: Columna rank_tip donde 1 es la propina mÃ¡s alta de cada dÃ­a.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = sns.load_dataset(\"tips\")\n",
    "df_tres =df_tres.sort_values(by=[\"day\",\"tip\"],ascending=[True,False])\n",
    "df_tres[\"ranking_tip\"] = df_tres.groupby(\"day\",observed=True)[\"total_bill\"].cumcount()+1\n",
    "df_tres.head(20)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. Diferencia entre registros consecutivos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la diferencia entre la venta del dÃ­a actual y la del dÃ­a anterior.\n",
    "âœï¸ Resultado esperado: Nueva columna diff_ventas con valores [NaN, 50, -20].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\"],\n",
    "    \"ventas\":[100,150,130]\n",
    "}\n",
    "df_cuatro = pd.DataFrame(diccionario_cuatro)\n",
    "df_cuatro[\"fecha\"] = pd.to_datetime(df_cuatro[\"fecha\"])\n",
    "# df_cuatro.head()\n",
    "\n",
    "df_cuatro[\"diff_ventas\"] = (df_cuatro[\"ventas\"]-df_cuatro[\"ventas\"].shift(1))\n",
    "df_cuatro.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. Percentil mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula el percentil 75 mÃ³vil de passengers en una ventana de 12 meses.\n",
    "âœï¸ Resultado esperado: Nueva columna con el valor del percentil 75 dentro de cada ventana mÃ³vil.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cinco = sns.load_dataset(\"flights\")\n",
    "# df_cinco.head()\n",
    "df_cinco[\"percentil_movil_75\"] = df_cinco[\"passengers\"].rolling(window=12).quantile(0.75)\n",
    "df_cinco.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293a774",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df5225c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (144, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year</th><th>month</th><th>passengers</th><th></th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>1949</td><td>&quot;Jan&quot;</td><td>112</td><td>null</td></tr><tr><td>1949</td><td>&quot;Feb&quot;</td><td>118</td><td>null</td></tr><tr><td>1949</td><td>&quot;Mar&quot;</td><td>132</td><td>null</td></tr><tr><td>1949</td><td>&quot;Apr&quot;</td><td>129</td><td>null</td></tr><tr><td>1949</td><td>&quot;May&quot;</td><td>121</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1960</td><td>&quot;Aug&quot;</td><td>606</td><td>null</td></tr><tr><td>1960</td><td>&quot;Sep&quot;</td><td>508</td><td>null</td></tr><tr><td>1960</td><td>&quot;Oct&quot;</td><td>461</td><td>null</td></tr><tr><td>1960</td><td>&quot;Nov&quot;</td><td>390</td><td>null</td></tr><tr><td>1960</td><td>&quot;Dec&quot;</td><td>432</td><td>535.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (144, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ year â”† month â”† passengers â”†       â”‚\n",
       "â”‚ ---  â”† ---   â”† ---        â”† ---   â”‚\n",
       "â”‚ i64  â”† str   â”† i64        â”† f64   â”‚\n",
       "â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ 1949 â”† Jan   â”† 112        â”† null  â”‚\n",
       "â”‚ 1949 â”† Feb   â”† 118        â”† null  â”‚\n",
       "â”‚ 1949 â”† Mar   â”† 132        â”† null  â”‚\n",
       "â”‚ 1949 â”† Apr   â”† 129        â”† null  â”‚\n",
       "â”‚ 1949 â”† May   â”† 121        â”† null  â”‚\n",
       "â”‚ â€¦    â”† â€¦     â”† â€¦          â”† â€¦     â”‚\n",
       "â”‚ 1960 â”† Aug   â”† 606        â”† null  â”‚\n",
       "â”‚ 1960 â”† Sep   â”† 508        â”† null  â”‚\n",
       "â”‚ 1960 â”† Oct   â”† 461        â”† null  â”‚\n",
       "â”‚ 1960 â”† Nov   â”† 390        â”† null  â”‚\n",
       "â”‚ 1960 â”† Dec   â”† 432        â”† 535.0 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "1. Promedio mÃ³vil de ventas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio mÃ³vil de 3 dÃ­as sobre la columna ventas.\n",
    "âœï¸ Resultado esperado: Nueva columna con el promedio de los Ãºltimos 3 dÃ­as (primeros 2 valores â†’ NaN).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_uno = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\"],\n",
    "    \"ventas\":[100,150,130,170,200]\n",
    "}\n",
    "df_uno = pl.LazyFrame(diccionario_uno)\n",
    "df_uno = df_uno.with_columns(\n",
    "    pl.col(\"fecha\").cast(pl.Date).alias(\"fecha\")\n",
    ")\n",
    "# df_uno.collect()\n",
    "df_uno = df_uno.with_columns(\n",
    "    pl.col(\"ventas\").rolling_mean(window_size=3).round(2).alias(\"promedio_movil_3D\")\n",
    ")\n",
    "df_uno.collect()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Suma acumulada (expanding sum)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: ObtÃ©n la suma acumulada de passengers por aÃ±o (year).\n",
    "âœï¸ Resultado esperado: Columna que acumule progresivamente los pasajeros en cada aÃ±o.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = pl.scan_csv(\"../datasets/flights.csv\",separator=\",\")\n",
    "# df_dos.collect()\n",
    "df_dos = df_dos.with_columns(\n",
    "    pl.col(\"passengers\").cum_sum().over(partition_by=\"year\").alias(\"passengers_acumulados_anualmente\")\n",
    ")\n",
    "df_dos.collect()\n",
    "\n",
    "\"\"\"\n",
    "3. Ranking de propinas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Asigna un ranking a cada registro de tip dentro de cada day, del mayor al menor.\n",
    "âœï¸ Resultado esperado: Columna rank_tip donde 1 es la propina mÃ¡s alta de cada dÃ­a.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = pl.scan_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_tres.collect()\n",
    "df_tres = df_tres.sort(by=[\"day\",\"tip\"],descending=[False,True])\n",
    "df_tres = df_tres.with_columns(\n",
    "    pl.col(\"tip\").cum_count().over(partition_by=\"day\").alias(\"ranking_tip\")\n",
    ")\n",
    "df_tres.collect()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. Diferencia entre registros consecutivos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la diferencia entre la venta del dÃ­a actual y la del dÃ­a anterior.\n",
    "âœï¸ Resultado esperado: Nueva columna diff_ventas con valores [NaN, 50, -20].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\"],\n",
    "    \"ventas\":[100,150,130]\n",
    "}\n",
    "\n",
    "df_cuatro = pl.LazyFrame(diccionario_cuatro)\n",
    "# df_cuatro.collect()\n",
    "df_cuatro = df_cuatro.with_columns(\n",
    "    pl.col(\"fecha\").cast(pl.Date).alias(\"fecha\")\n",
    ")\n",
    "df_cuatro = df_cuatro.with_columns(\n",
    "    pl.col(\"ventas\").shift(1).alias(\"ventas_dia_anterior\")\n",
    ").with_columns(\n",
    "    (pl.col(\"ventas\") - pl.col(\"ventas_dia_anterior\")).alias(\"diferencia\")\n",
    ")\n",
    "df_cuatro.collect()\n",
    "\n",
    "\"\"\"\n",
    "5. Percentil mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula el percentil 75 mÃ³vil de passengers en una ventana de 12 meses.\n",
    "âœï¸ Resultado esperado: Nueva columna con el valor del percentil 75 dentro de cada ventana mÃ³vil.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_cinco = pl.scan_csv(\"../datasets/flights.csv\",separator=\",\")\n",
    "# df_cinco.collect()\n",
    "df_cinco = df_cinco.with_columns(\n",
    "    pl.col(\"passengers\").rolling_quantile(quantile=0.75,window_size=12).over(partition_by=\"year\").alias(\"\")\n",
    ")\n",
    "df_cinco.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2768147",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5315f",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1257bf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>ventas</th>\n",
       "      <th>gastos</th>\n",
       "      <th>corr_movil_3D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>140</td>\n",
       "      <td>110</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>150</td>\n",
       "      <td>120</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fecha  ventas  gastos  corr_movil_3D\n",
       "0  2024-01-01     100      80            NaN\n",
       "1  2024-01-02     120      90            NaN\n",
       "2  2024-01-03     130     100           0.98\n",
       "3  2024-01-04     140     110           1.00\n",
       "4  2024-01-05     150     120           1.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "6. Ranking y Dense Rank por grupo\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Dentro de cada pclass, asigna rank y dense_rank a los pasajeros segÃºn fare (de mayor a menor).\n",
    "âœï¸ Resultado esperado: Dos columnas (rank, dense_rank) que muestren diferencias cuando hay empates.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_seis = sns.load_dataset(\"titanic\")\n",
    "# df_seis.head()\n",
    "df_seis = df_seis.sort_values(by=[\"fare\",\"pclass\"],ascending=[False,True])\n",
    "df_seis[\"rank_pclass\"] = df_seis.groupby(\"pclass\")[\"fare\"].cumcount() + 1\n",
    "df_seis[\"dense_rank_pclass\"] = df_seis.groupby(\"pclass\")[\"fare\"].rank(method=\"dense\")\n",
    "df_seis.head()\n",
    "\n",
    "\"\"\"\n",
    "7. Promedio mÃ³vil agrupado\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio mÃ³vil de 6 meses de passengers para cada year.\n",
    "âœï¸ Resultado esperado: Nueva columna con el promedio mÃ³vil dentro de cada aÃ±o.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = sns.load_dataset(\"flights\")\n",
    "# df_siete.head()\n",
    "df_siete[\"promedio_movil_6D\"] = df_siete.groupby(\"year\")[\"passengers\"].rolling(window=6).mean().round(2).reset_index(level=0,drop=True)\n",
    "df_siete.head(12)\n",
    "\n",
    "\"\"\"\n",
    "8. Lag de precios por producto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna precio_anterior que muestre el precio del dÃ­a anterior para cada producto.\n",
    "âœï¸ Resultado esperado: Nueva columna con el valor desplazado un dÃ­a hacia atrÃ¡s dentro de cada producto.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_ocho = {\n",
    "    \"producto\":[\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"],\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-01\",\"2024-01-02\",\"2024-01-03\"],\n",
    "    \"precio\":[10,12,15,20,18,22]\n",
    "}\n",
    "df_ocho = pd.DataFrame(diccionario_ocho)\n",
    "# df_ocho.head()\n",
    "df_ocho[\"fecha\"] = pd.to_datetime(df_ocho[\"fecha\"])\n",
    "# df_ocho.head()\n",
    "\n",
    "df_ocho[\"precio_anterior\"] = df_ocho[\"precio\"].shift(1)\n",
    "df_ocho.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "9. Lead y diferencia futura\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Para cada day, crea una columna con la propina (tip) del siguiente registro (lead) y otra con la diferencia con la actual.\n",
    "âœï¸ Resultado esperado: Columnas next_tip y diff_next_tip con los valores adelantados y su diferencia.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_nueve = sns.load_dataset(\"tips\")\n",
    "# df_nueve.head()\n",
    "df_nueve[\"next_tip\"] = df_nueve[\"tip\"].shift(-1)\n",
    "# df_nueve.head()\n",
    "df_nueve[\"diff_next_tip\"] = (df_nueve[\"next_tip\"] - df_nueve[\"tip\"])\n",
    "df_nueve.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "10. CorrelaciÃ³n mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la correlaciÃ³n mÃ³vil de 3 dÃ­as entre ventas y gastos.\n",
    "âœï¸ Resultado esperado: Nueva columna con valores que indiquen la relaciÃ³n lineal en cada ventana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\",\"2024-01-06\"],\n",
    "    \"ventas\":[100,120,130,140,150,160],\n",
    "    \"gastos\":[80,90,100,110,120,130]\n",
    "}\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez[\"corr_movil_3D\"] = df_diez[\"ventas\"].rolling(window=3).corr(df_diez[\"gastos\"]).round(2)\n",
    "df_diez.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55d5e0",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e448bb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fecha</th><th>ventas</th><th>gastos</th><th>correlacion_movil_3D</th></tr><tr><td>date</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-01-01</td><td>100</td><td>80</td><td>null</td></tr><tr><td>2024-01-02</td><td>120</td><td>90</td><td>null</td></tr><tr><td>2024-01-03</td><td>130</td><td>100</td><td>0.982</td></tr><tr><td>2024-01-04</td><td>140</td><td>110</td><td>1.0</td></tr><tr><td>2024-01-05</td><td>150</td><td>120</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ fecha      â”† ventas â”† gastos â”† correlacion_movil_3D â”‚\n",
       "â”‚ ---        â”† ---    â”† ---    â”† ---                  â”‚\n",
       "â”‚ date       â”† i64    â”† i64    â”† f64                  â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2024-01-01 â”† 100    â”† 80     â”† null                 â”‚\n",
       "â”‚ 2024-01-02 â”† 120    â”† 90     â”† null                 â”‚\n",
       "â”‚ 2024-01-03 â”† 130    â”† 100    â”† 0.982                â”‚\n",
       "â”‚ 2024-01-04 â”† 140    â”† 110    â”† 1.0                  â”‚\n",
       "â”‚ 2024-01-05 â”† 150    â”† 120    â”† 1.0                  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. Ranking y Dense Rank por grupo\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Dentro de cada pclass, asigna rank y dense_rank a los pasajeros segÃºn fare (de mayor a menor).\n",
    "âœï¸ Resultado esperado: Dos columnas (rank, dense_rank) que muestren diferencias cuando hay empates.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.scan_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_seis.collect()\n",
    "df_seis = df_seis.sort(by=[\"pclass\",\"fare\"],descending=[False,True])\n",
    "# df_seis.collect()\n",
    "df_seis = df_seis.with_columns(\n",
    "    pl.col(\"fare\").rank(method=\"ordinal\",descending=False).alias(\"rank_Fare\"),\n",
    "    pl.col(\"fare\").rank(method=\"dense\",descending=False).alias(\"rank_dense_Fare\")\n",
    ").sort(by=[\"rank_Fare\",\"rank_dense_Fare\"],descending=[True,True])\n",
    "df_seis.collect()\n",
    "\n",
    "\"\"\"\n",
    "7. Promedio mÃ³vil agrupado\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula el promedio mÃ³vil de 6 meses de passengers para cada year.\n",
    "âœï¸ Resultado esperado: Nueva columna con el promedio mÃ³vil dentro de cada aÃ±o.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = pl.scan_csv(\"../datasets/flights.csv\",separator=\",\")\n",
    "# df_siete.collect()\n",
    "df_siete = df_siete.with_columns(\n",
    "    pl.col(\"passengers\").rolling_mean(window_size=6).over(partition_by=\"year\").alias(\"promedio_movil_6D\")\n",
    ")\n",
    "df_siete.collect()\n",
    "\n",
    "\"\"\"\n",
    "8. Lag de precios por producto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna precio_anterior que muestre el precio del dÃ­a anterior para cada producto.\n",
    "âœï¸ Resultado esperado: Nueva columna con el valor desplazado un dÃ­a hacia atrÃ¡s dentro de cada producto.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_ocho = {\n",
    "    \"producto\":[\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"],\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-01\",\"2024-01-02\",\"2024-01-03\"],\n",
    "    \"precio\":[10,12,15,20,18,22]\n",
    "}\n",
    "\n",
    "df_ocho = pl.LazyFrame(diccionario_ocho)\n",
    "df_ocho = df_ocho.with_columns(\n",
    "    pl.col(\"fecha\").cast(pl.Date).alias(\"fecha\")\n",
    ")\n",
    "# df_ocho.collect()\n",
    "\n",
    "df_ocho = df_ocho.with_columns(\n",
    "    pl.col(\"precio\").shift(1).alias(\"precio_anterior\")\n",
    ")\n",
    "df_ocho.collect()\n",
    "\n",
    "\"\"\"\n",
    "9. Lead y diferencia futura\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Para cada day, crea una columna con la propina (tip) del siguiente registro (lead) y otra con la diferencia con la actual.\n",
    "âœï¸ Resultado esperado: Columnas next_tip y diff_next_tip con los valores adelantados y su diferencia.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_nueve = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_nueve.head()\n",
    "df_nueve = df_nueve.sort(by=\"day\")\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.col(\"tip\").shift(-1).over(partition_by=\"day\").alias(\"tip_dia_posterior\")\n",
    ")\n",
    "# df_nueve.head()\n",
    "\n",
    "\"\"\"\n",
    "10. CorrelaciÃ³n mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la correlaciÃ³n mÃ³vil de 3 dÃ­as entre ventas y gastos.\n",
    "âœï¸ Resultado esperado: Nueva columna con valores que indiquen la relaciÃ³n lineal en cada ventana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\",\"2024-01-06\"],\n",
    "    \"ventas\":[100,120,130,140,150,160],\n",
    "    \"gastos\":[80,90,100,110,120,130]\n",
    "}\n",
    "\n",
    "df_diez = pl.DataFrame(diccionario_diez)\n",
    "# df_diez.collect()\n",
    "df_diez = df_diez.with_columns(\n",
    "    pl.col(\"fecha\").cast(pl.Date).alias(\"fecha\")\n",
    ")\n",
    "# df_diez.collect()\n",
    "df_diez_correlacion = df_diez.with_columns(\n",
    "    pl.rolling_corr(a=pl.col(\"ventas\"),b=pl.col(\"gastos\"),window_size=3).round(3).alias(\"correlacion_movil_3D\")\n",
    ")\n",
    "df_diez_correlacion.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7135f",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391e904",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763b7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>passengers</th>\n",
       "      <th>passenger_last_month</th>\n",
       "      <th>diff_last_month</th>\n",
       "      <th>var_porc_last_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949</td>\n",
       "      <td>Jan</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949</td>\n",
       "      <td>Feb</td>\n",
       "      <td>118</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>Mar</td>\n",
       "      <td>132</td>\n",
       "      <td>118.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949</td>\n",
       "      <td>Apr</td>\n",
       "      <td>129</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949</td>\n",
       "      <td>May</td>\n",
       "      <td>121</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1949</td>\n",
       "      <td>Jun</td>\n",
       "      <td>135</td>\n",
       "      <td>121.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1949</td>\n",
       "      <td>Jul</td>\n",
       "      <td>148</td>\n",
       "      <td>135.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1949</td>\n",
       "      <td>Aug</td>\n",
       "      <td>148</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1949</td>\n",
       "      <td>Sep</td>\n",
       "      <td>136</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1949</td>\n",
       "      <td>Oct</td>\n",
       "      <td>119</td>\n",
       "      <td>136.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1949</td>\n",
       "      <td>Nov</td>\n",
       "      <td>104</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1949</td>\n",
       "      <td>Dec</td>\n",
       "      <td>118</td>\n",
       "      <td>104.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1950</td>\n",
       "      <td>Jan</td>\n",
       "      <td>115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1950</td>\n",
       "      <td>Feb</td>\n",
       "      <td>126</td>\n",
       "      <td>115.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1950</td>\n",
       "      <td>Mar</td>\n",
       "      <td>141</td>\n",
       "      <td>126.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1950</td>\n",
       "      <td>Apr</td>\n",
       "      <td>135</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1950</td>\n",
       "      <td>May</td>\n",
       "      <td>125</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1950</td>\n",
       "      <td>Jun</td>\n",
       "      <td>149</td>\n",
       "      <td>125.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1950</td>\n",
       "      <td>Jul</td>\n",
       "      <td>170</td>\n",
       "      <td>149.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1950</td>\n",
       "      <td>Aug</td>\n",
       "      <td>170</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year month  passengers  passenger_last_month  diff_last_month  \\\n",
       "0   1949   Jan         112                   NaN              NaN   \n",
       "1   1949   Feb         118                 112.0              6.0   \n",
       "2   1949   Mar         132                 118.0             14.0   \n",
       "3   1949   Apr         129                 132.0             -3.0   \n",
       "4   1949   May         121                 129.0             -8.0   \n",
       "5   1949   Jun         135                 121.0             14.0   \n",
       "6   1949   Jul         148                 135.0             13.0   \n",
       "7   1949   Aug         148                 148.0              0.0   \n",
       "8   1949   Sep         136                 148.0            -12.0   \n",
       "9   1949   Oct         119                 136.0            -17.0   \n",
       "10  1949   Nov         104                 119.0            -15.0   \n",
       "11  1949   Dec         118                 104.0             14.0   \n",
       "12  1950   Jan         115                   NaN              NaN   \n",
       "13  1950   Feb         126                 115.0             11.0   \n",
       "14  1950   Mar         141                 126.0             15.0   \n",
       "15  1950   Apr         135                 141.0             -6.0   \n",
       "16  1950   May         125                 135.0            -10.0   \n",
       "17  1950   Jun         149                 125.0             24.0   \n",
       "18  1950   Jul         170                 149.0             21.0   \n",
       "19  1950   Aug         170                 170.0              0.0   \n",
       "\n",
       "    var_porc_last_month  \n",
       "0                   NaN  \n",
       "1                   5.0  \n",
       "2                  12.0  \n",
       "3                  -2.0  \n",
       "4                  -6.0  \n",
       "5                  12.0  \n",
       "6                  10.0  \n",
       "7                   0.0  \n",
       "8                  -8.0  \n",
       "9                 -12.0  \n",
       "10                -13.0  \n",
       "11                 13.0  \n",
       "12                  NaN  \n",
       "13                 10.0  \n",
       "14                 12.0  \n",
       "15                 -4.0  \n",
       "16                 -7.0  \n",
       "17                 19.0  \n",
       "18                 14.0  \n",
       "19                  0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "11. Ventana con mÃºltiples funciones\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula para passengers:\n",
    "\n",
    "    * Promedio mÃ³vil (3 meses)\n",
    "\n",
    "    * DesviaciÃ³n estÃ¡ndar mÃ³vil (3 meses)\n",
    "\n",
    "    * Valor mÃ¡ximo mÃ³vil (3 meses)\n",
    "\n",
    "âœï¸ Resultado esperado: Tres columnas nuevas reflejando las mÃ©tricas por ventana temporal.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_once = sns.load_dataset(\"flights\")\n",
    "# df_once.head()\n",
    "df_once[\"promedio_movil_3M\"] = df_once.groupby([\"year\"],observed=True)[\"passengers\"].rolling(window=3).mean().round(2).reset_index(level=0,drop=True)\n",
    "df_once[\"std_movil_3M\"] = df_once.groupby([\"year\"],observed=True)[\"passengers\"].rolling(window=3).std().round(2).reset_index(level=0,drop=True)\n",
    "df_once[\"max_movil_3M\"] = df_once.groupby([\"year\"],observed=True)[\"passengers\"].rolling(window=3).max().reset_index(level=0,drop=True)\n",
    "df_once.head(20)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. Ranking dinÃ¡mico y top-N por grupo\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Dentro de cada day, determina las 2 mayores propinas usando ranking.\n",
    "âœï¸ Resultado esperado: Subconjunto filtrado con los top 2 registros por dÃ­a.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_doce = sns.load_dataset(\"tips\")\n",
    "# df_doce.head()\n",
    "df_doce = df_doce.sort_values(by=[\"day\",\"tip\"],ascending=[True,False])\n",
    "df_doce[\"rank_tip_day\"] = df_doce.groupby(\"day\",observed=True)[\"tip\"].cumcount()+1\n",
    "# df_doce.head(20)\n",
    "\n",
    "df_doce_max_tip = df_doce.query(\"rank_tip_day<=2\")\n",
    "df_doce_max_tip.head(10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. Diferencia porcentual entre registros consecutivos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula la variaciÃ³n porcentual de passengers respecto al mes anterior.\n",
    "âœï¸ Resultado esperado: Columna con el porcentaje de cambio entre meses consecutivos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_trece = sns.load_dataset(\"flights\")\n",
    "# df_trece.head()\n",
    "df_trece[\"passenger_last_month\"] = df_trece.groupby(\"year\",observed=True)[\"passengers\"].shift(1)\n",
    "df_trece[\"diff_last_month\"] = df_trece[\"passengers\"] - df_trece[\"passenger_last_month\"]\n",
    "df_trece[\"var_porc_last_month\"] = round((df_trece[\"diff_last_month\"]/df_trece[\"passenger_last_month\"]),2)*100\n",
    "df_trece.head(20)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "14. Percentiles dentro de ventanas agrupadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Calcula el percentil 90 del precio (price) para cada combinaciÃ³n de cut y color dentro de una ventana.\n",
    "âœï¸ Resultado esperado: Nueva columna que indique el valor del percentil 90 del grupo.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_catorce = sns.load_dataset(\"diamonds\")\n",
    "# df_catorce.head()\n",
    "df_catorce[\"perc_90_price\"] = df_catorce.groupby([\"cut\",\"color\"],level=0,observed=True)[\"price\"].quantile(0.90).reset_index(level=0,drop=True)\n",
    "df_catorce.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "15. CÃ¡lculo de covarianza mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la covarianza mÃ³vil de 3 dÃ­as entre ventas y gastos.\n",
    "âœï¸ Resultado esperado: Columna con valores que representen cÃ³mo varÃ­an conjuntamente ambas mÃ©tricas en el tiempo.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_quince = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\"],\n",
    "    \"ventas\":[100,150,200,250,300],\n",
    "    \"gastos\":[80,130,190,240,310]\n",
    "}\n",
    "\n",
    "df_quince = pd.DataFrame(diccionario_quince)\n",
    "# df_quince.head()\n",
    "df_quince[\"fecha\"] = pd.to_datetime(df_quince[\"fecha\"])\n",
    "df_quince[\"covarianza_movil_3D\"] = df_quince[\"ventas\"].rolling(window=3).cov(df_quince[\"gastos\"]).round(2).reset_index(level=0,drop=True)\n",
    "df_quince.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3a6d6",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80ce3371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fecha</th><th>ventas</th><th>gastos</th><th>cov_3D</th></tr><tr><td>date</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-01-01</td><td>100</td><td>80</td><td>null</td></tr><tr><td>2024-01-02</td><td>150</td><td>130</td><td>null</td></tr><tr><td>2024-01-03</td><td>200</td><td>190</td><td>2750.0</td></tr><tr><td>2024-01-04</td><td>250</td><td>240</td><td>2750.0</td></tr><tr><td>2024-01-05</td><td>300</td><td>310</td><td>3000.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ fecha      â”† ventas â”† gastos â”† cov_3D â”‚\n",
       "â”‚ ---        â”† ---    â”† ---    â”† ---    â”‚\n",
       "â”‚ date       â”† i64    â”† i64    â”† f64    â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2024-01-01 â”† 100    â”† 80     â”† null   â”‚\n",
       "â”‚ 2024-01-02 â”† 150    â”† 130    â”† null   â”‚\n",
       "â”‚ 2024-01-03 â”† 200    â”† 190    â”† 2750.0 â”‚\n",
       "â”‚ 2024-01-04 â”† 250    â”† 240    â”† 2750.0 â”‚\n",
       "â”‚ 2024-01-05 â”† 300    â”† 310    â”† 3000.0 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "11. Ventana con mÃºltiples funciones\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula para passengers:\n",
    "\n",
    "    * Promedio mÃ³vil (3 meses)\n",
    "\n",
    "    * DesviaciÃ³n estÃ¡ndar mÃ³vil (3 meses)\n",
    "\n",
    "    * Valor mÃ¡ximo mÃ³vil (3 meses)\n",
    "\n",
    "âœï¸ Resultado esperado: Tres columnas nuevas reflejando las mÃ©tricas por ventana temporal.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_once = pl.read_csv(\"../datasets/flights.csv\",separator=\",\")\n",
    "# df_once.head()\n",
    "df_once = df_once.with_columns(\n",
    "    pl.col(\"passengers\").rolling_mean(window_size=3).round(2).alias(\"avg_movil_3D\"),\n",
    "    pl.col(\"passengers\").rolling_std(window_size=3).round(2).alias(\"std_movil_3D\"),\n",
    "    pl.col(\"passengers\").rolling_max(window_size=3).alias(\"max_movil_3D\")\n",
    ")\n",
    "df_once.head()\n",
    "\n",
    "\"\"\"\n",
    "12. Ranking dinÃ¡mico y top-N por grupo\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Dentro de cada day, determina las 2 mayores propinas usando ranking.\n",
    "âœï¸ Resultado esperado: Subconjunto filtrado con los top 2 registros por dÃ­a.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_doce = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce = df_doce.sort(by=[\"day\",\"tip\"],descending=[False,True])\n",
    "\n",
    "df_doce = df_doce.with_columns(\n",
    "    pl.col(\"tip\").cum_count().over(partition_by=\"day\").alias(\"ranking_tip_day\")\n",
    ")\n",
    "df_doce.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. Diferencia porcentual entre registros consecutivos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Flights\n",
    "ğŸ—’ï¸ Enunciado: Calcula la variaciÃ³n porcentual de passengers respecto al mes anterior.\n",
    "âœï¸ Resultado esperado: Columna con el porcentaje de cambio entre meses consecutivos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_trece = pl.read_csv(\"../datasets/flights.csv\",separator=\",\")\n",
    "# df_trece.head()\n",
    "df_trece = df_trece.with_columns(\n",
    "    pl.col(\"passengers\").shift(1).over(partition_by=\"year\").alias(\"passengers_last_month\")\n",
    ").with_columns(\n",
    "    (((pl.col(\"passengers\") - pl.col(\"passengers_last_month\"))/pl.col(\"passengers_last_month\"))*100).round(2).alias(\"varianza_porcentual\")\n",
    ")\n",
    "df_trece.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "14. Percentiles dentro de ventanas agrupadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Calcula el percentil 90 del precio (price) para cada combinaciÃ³n de cut y color dentro de una ventana.\n",
    "âœï¸ Resultado esperado: Nueva columna que indique el valor del percentil 90 del grupo.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "df_catorce = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_catorce.head()\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    pl.col(\"price\").quantile(0.9).over(partition_by=[\"cut\",\"color\"]).alias(\"percentil_90_price\")\n",
    ")\n",
    "df_catorce.head()\n",
    "\n",
    "\"\"\"\n",
    "15. CÃ¡lculo de covarianza mÃ³vil\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula la covarianza mÃ³vil de 3 dÃ­as entre ventas y gastos.\n",
    "âœï¸ Resultado esperado: Columna con valores que representen cÃ³mo varÃ­an conjuntamente ambas mÃ©tricas en el tiempo.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_quince = {\n",
    "    \"fecha\":[\"2024-01-01\",\"2024-01-02\",\"2024-01-03\",\"2024-01-04\",\"2024-01-05\"],\n",
    "    \"ventas\":[100,150,200,250,300],\n",
    "    \"gastos\":[80,130,190,240,310]\n",
    "}\n",
    "\n",
    "df_quince = pl.DataFrame(diccionario_quince)\n",
    "# df_quince.head()\n",
    "df_quince = df_quince.with_columns(\n",
    "    pl.col(\"fecha\").cast(pl.Date).alias(\"fecha\")\n",
    ")\n",
    "# df_quince.head()\n",
    "df_quince = df_quince.with_columns(\n",
    "    pl.rolling_cov(\"ventas\",\"gastos\",window_size=3).alias(\"cov_3D\")\n",
    ")\n",
    "df_quince.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
