{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e02fad",
   "metadata": {},
   "source": [
    "### 📘 Talleres de Ingeniería de Datos con Pandas y Polars 🐼🐻‍❄️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db83f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "👨‍💻 Autor: Brayan Neciosup  \n",
    "📍 Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "🔗 LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "💻 GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "📚 Serie: Fundamentos de Pandas y Polars   \n",
    "📓 Estos talleres constarán de 3 niveles (Básico-Intermedio-Avanzado)   \n",
    "🔍 Abarcará temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "📝 Cada ejercicio presenta su enunciado, dataset, resultado esperado y solución.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cd600",
   "metadata": {},
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACIÓN DE DATOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90306f5b",
   "metadata": {},
   "source": [
    "##### 🥉 NIVEL BÁSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1a1a3",
   "metadata": {},
   "source": [
    "###### PANDAS 🐼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51d9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. Detección de valores nulos en columnas principales\n",
    "\n",
    "🗃️ Dataset: TITANIC\n",
    "🗒️ Enunciado: Identifica cuántos valores faltantes hay en las columnas age, embarked y deck.\n",
    "✍️ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_uno = sns.load_dataset(\"titanic\")\n",
    "# df_uno.head()\n",
    "# df_uno[[\"age\",\"embarked\",\"deck\"]].isnull().sum() ## ➡️ Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n",
    "\n",
    "\"\"\"\n",
    "2. Eliminación de filas duplicadas\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Elimina las filas duplicadas y conserva solo la primera aparición de cada registro.\n",
    "✍️ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"María\",\"Pedro\",\"Pedro\",\"Sofía\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pd.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "# df_dos.shape[0] ## ➡️ Cantidad de datos: 7\n",
    "df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"],keep=\"first\",inplace=True)\n",
    "# df_dos.head()\n",
    "df_dos.shape[0] ## ➡️ Cantidad de datos: 5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "🗃️ Dataset: PENGUINS\n",
    "🗒️ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "✍️ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_tres = sns.load_dataset(\"penguins\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].isnull().sum() ## ➡️ Cantidad de datos nulos: 2\n",
    "media_bill_length_mm = float(np.mean(df_tres[\"bill_length_mm\"].dropna()).round(2))\n",
    "df_tres.fillna({\"bill_length_mm\":media_bill_length_mm},inplace=True)\n",
    "df_tres[\"bill_length_mm\"].isnull().sum() ## ➡️ Cantidad de datos nulos: 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3644744",
   "metadata": {},
   "source": [
    "###### POLARS 🐻‍❄️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f88ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. Detección de valores nulos en columnas principales\n",
    "\n",
    "🗃️ Dataset: TITANIC\n",
    "🗒️ Enunciado: Identifica cuántos valores faltantes hay en las columnas age, embarked y deck.\n",
    "✍️ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_uno = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_uno.head()\n",
    "df_uno.null_count()\n",
    "\n",
    "\"\"\"\n",
    "2. Eliminación de filas duplicadas\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Elimina las filas duplicadas y conserva solo la primera aparición de cada registro.\n",
    "✍️ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"María\",\"Pedro\",\"Pedro\",\"Sofía\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pl.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "df_dos = df_dos.unique(keep=\"first\")\n",
    "df_dos.head()\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "🗃️ Dataset: PENGUINS\n",
    "🗒️ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "✍️ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_tres = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].null_count() ## ➡️ Cantidad Nulos: 2\n",
    "media_bill_length_mm = df_tres[\"bill_length_mm\"].mean().__round__(2)\n",
    "media_bill_length_mm\n",
    "df_tres = df_tres.with_columns(\n",
    "    pl.col(\"bill_length_mm\").fill_null(media_bill_length_mm).alias(\"bill_length_mm\")\n",
    ")\n",
    "# df_tres.head()\n",
    "df_tres[\"bill_length_mm\"].null_count() ## ➡️ Cantidad Nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87889e12",
   "metadata": {},
   "source": [
    "##### 🥈 NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f9c68",
   "metadata": {},
   "source": [
    "###### PANDAS 🐼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "4. Imputación condicional de valores faltantes\n",
    "\n",
    "🗃️ Dataset: TITANIC\n",
    "🗒️ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "✍️ Resultado esperado: columna age sin valores nulos, imputada según clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_cuatro = sns.load_dataset(\"titanic\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].isnull().sum() ## ➡️ Cantidad de datos nulos: 177\n",
    "edad_promedio_por_clase = df_cuatro.groupby(\"pclass\",as_index=False,observed=True)[\"age\"].mean().round(2)\n",
    "edad_promedio_por_clase\n",
    "# df_cuatro[\"age\"].isnull().sum() ## ➡️ Cantidad de datos nulos: 177\n",
    "\n",
    "df_cuatro_clean = df_cuatro.copy()\n",
    "df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1] = df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==1')[\"age\"][0])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==2] = df_cuatro[df_cuatro[\"pclass\"]==2].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==2')[\"age\"][1])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==3] = df_cuatro[df_cuatro[\"pclass\"]==3].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query(\"pclass==3\")[\"age\"][2])\n",
    "})\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==2].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==3].isnull().sum() ## Cantidad: 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. Detección de outliers usando IQR\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Identifica los valores de ventas que son outliers según el rango intercuartílico (IQR).\n",
    "✍️ Resultado esperado: listado de los productos que presentan valores anómalos.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "# df_cinco.describe()\n",
    "q1_ventas = float(np.quantile(df_cinco[\"ventas\"],0.25))\n",
    "q3_ventas = float(np.quantile(df_cinco[\"ventas\"],0.75))\n",
    "iqr_ventas = q3_ventas-q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "df_cinco_outliers = df_cinco[(df_cinco[\"ventas\"]<lower_bound_ventas) | (df_cinco[\"ventas\"]>upper_bound_ventas)]\n",
    "# df_cinco_outliers.head()\n",
    "df_cinco_outliers.shape[0] ## CANTIDAD DE OUTLIERS EN COLUMNA VENTAS: 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. Eliminación selectiva de duplicados\n",
    "\n",
    "🗃️ Dataset: Diamonds\n",
    "🗒️ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "✍️ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_seis = sns.load_dataset(\"diamonds\")\n",
    "df_seis.head()\n",
    "# df_seis.shape[0] ## 53940 DATOS\n",
    "# df_seis[\"carat\"].duplicated().sum() ## 53 667 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis[\"price\"].duplicated().sum() ## 42 338 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis.drop_duplicates(subset=[\"carat\",\"price\"],inplace=True)\n",
    "# df_seis.shape[0] ## 28988 DATOS DESPUES DE REMOVER DUPLICADOS\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolación\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolación lineal.\n",
    "✍️ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_siete = {\n",
    "    \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    "    \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pd.DataFrame(diccionario_siete)\n",
    "df_siete_interpolado = df_siete.copy()\n",
    "df_siete_interpolado = df_siete_interpolado.interpolate(method=\"linear\")\n",
    "df_siete_interpolado.head()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "🗃️ Dataset: Penguins\n",
    "🗒️ Enunciado: Calcula el número de registros que tienen valores nulos simultáneamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "✍️ Resultado esperado: un número entero que indique cuántos registros cumplen esta condición.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_ocho = sns.load_dataset(\"penguins\")\n",
    "# df_ocho.head()\n",
    "df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].isnull().sum() ## ⬅️ Cantidad de datos nulos en ambas columnas (bill_length_mm:2 - bill_depth_mm: 2).\n",
    "df_ocho_nulos = df_ocho[(df_ocho[\"bill_length_mm\"].isnull()==True) & (df_ocho[\"bill_depth_mm\"].isnull()==True)]\n",
    "df_ocho_nulos.shape[0] ## ⬅️ Cantidad de valores nulos simultaneos en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo 👍.\n",
    "df_example = pd.DataFrame(data=[[1,None],[None,2],[None,None]],columns=[\"A\",\"B\"])\n",
    "# df_example.head() ## ⬅️ Como podremos observar hay un match en dos registros de la fila A y B que son nulos.\n",
    "cantidad_nulos = df_example[(df_example[\"A\"].isnull()==True) & (df_example[\"B\"].isnull()==True)]\n",
    "cantidad_nulos.shape[0] ## ⬅️ Cantidad de valores nulos simultaneos en ambas columnas: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d7d0b",
   "metadata": {},
   "source": [
    "###### POLARS 🐻‍❄️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bbcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "4. Imputación condicional de valores faltantes\n",
    "\n",
    "🗃️ Dataset: TITANIC\n",
    "🗒️ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "✍️ Resultado esperado: columna age sin valores nulos, imputada según clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_cuatro = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].null_count() ## ➡️ Cantidad de datos nulos: 177\n",
    "def llenar_nulos_pclass_edad(dataframe):\n",
    "    df = dataframe\n",
    "    edad_promedio_por_clase = df_cuatro.group_by(\"pclass\").agg(\n",
    "        pl.col(\"age\").drop_nulls().mean().round(2).alias(\"avg_age_pclass\")\n",
    "    )\n",
    "    \n",
    "    for pclass,media in edad_promedio_por_clase.iter_rows():\n",
    "        df = df.with_columns(\n",
    "            pl.when(\n",
    "                (pl.col(\"pclass\")==pclass) & (pl.col(\"age\").is_null())\n",
    "            ).then(pl.lit(media))\n",
    "            .otherwise(pl.col(\"age\")).alias(\"age\")\n",
    "        )\n",
    "    return df\n",
    "df_cuatro_clean = llenar_nulos_pclass_edad(df_cuatro)\n",
    "df_cuatro_clean[[\"age\"]].null_count() ## ➡️ Cantidad de datos nulos: 0 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. Detección de outliers usando IQR\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Identifica los valores de ventas que son outliers según el rango intercuartílico (IQR).\n",
    "✍️ Resultado esperado: listado de los productos que presentan valores anómalos.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "q1_ventas = df_cinco[\"ventas\"].quantile(0.25)\n",
    "q1_ventas\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. Eliminación selectiva de duplicados\n",
    "\n",
    "🗃️ Dataset: Diamonds\n",
    "🗒️ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "✍️ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_seis = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_seis.head()\n",
    "df_seis = df_seis.unique(subset=[\"carat\",\"price\"],keep=\"first\")\n",
    "df_seis.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolación\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolación lineal.\n",
    "✍️ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "from datetime import date\n",
    "diccionario_siete  = {\n",
    " \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pl.DataFrame(diccionario_siete)\n",
    "df_siete = df_siete.select(pl.col(\"fecha\"),pl.col(\"temperatura\").interpolate().alias(\"temperatura\"))\n",
    "df_siete.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "🗃️ Dataset: Penguins\n",
    "🗒️ Enunciado: Calcula el número de registros que tienen valores nulos simultáneamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "✍️ Resultado esperado: un número entero que indique cuántos registros cumplen esta condición.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_ocho = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_ocho.head()\n",
    "# df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].null_count() ## ➡️ Cantidad de Valores Nulos\n",
    "nulos_consecutivos = df_ocho.filter(\n",
    "    (pl.col(\"bill_length_mm\").is_null() & pl.col(\"bill_depth_mm\").is_null())\n",
    ")\n",
    "# nulos_consecutivos.head() ## ⬅️ Verificamos nulos simulares en ambas columnas\n",
    "# nulos_consecutivos.shape[0] ## ⬅️ Cantidad de nulos simulares en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo 👍.\n",
    "df_example = pl.DataFrame(data=[[1,None],[None,2],[None,None]],schema=[\"A\",\"B\"],orient=\"row\")\n",
    "# df_example.head() ## ⬅️ Verificamos que existen nulos en columnas similares\n",
    "nulos_consecutivos_2 = df_example.filter(\n",
    "    (pl.col(\"A\").is_null() & pl.col(\"B\").is_null())\n",
    ")\n",
    "# nulos_consecutivos_2.head() ## ⬅️ Verificamos nulos simulares en ambas columnas\n",
    "nulos_consecutivos_2.shape[0] ## ⬅️ Cantidad de nulos simulares en ambas columnas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db608e",
   "metadata": {},
   "source": [
    "##### 🥇 NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a022a",
   "metadata": {},
   "source": [
    "###### PANDAS 🐼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a91a88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ciudad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ciudad\n",
       "0   Lima\n",
       "1   Lima\n",
       "2   Lima\n",
       "3   Lima"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. Winsorización de outliers\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Aplica winsorización al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "✍️ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "💡 La winsorinización permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 12, 15, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pd.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## ➡️ Quartil 5%: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## ➡️ Quartil 95%: 170.0\n",
    "# limite_5_pct_superior\n",
    "condiciones_valores = [\n",
    "    (df_nueve[\"nota\"]<limite_5_pct_inferior), ## Condicina que, si el valor es menor al límite 5% inferior\n",
    "    (df_nueve[\"nota\"]>limite_5_pct_superior)  ## Condicina que, si el valor es mayor al límite 95% superior\n",
    "]\n",
    "values = np.array([limite_5_pct_inferior,limite_5_pct_superior]) ## Se establece los valores de las condiciones\n",
    "df_nueve_winzorizacion = df_nueve.copy()\n",
    "df_nueve_winzorizacion[\"nota_winsorizacion\"] = np.select(condiciones_valores,values,default=df_nueve[\"nota\"])\n",
    "df_nueve_winzorizacion.head(7)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "10. Detección de inconsistencias de tipado en columnas\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Identifica las filas con tipos incorrectos  y conviértelas al tipo correcto.\n",
    "✍️ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "df_diez[\"ID\"] = df_diez[\"ID\"].astype(dtype=\"int\") ## Casteamos el tipo de dato a int\n",
    "df_diez[\"Venta\"] = df_diez[\"Venta\"].astype(dtype=\"float\") ## Casteamos el tipo de dato a float\n",
    "# df_diez.head()\n",
    "df_diez.dtypes ##  ✅ Verificamos los tipos de dato correctamente casteados.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "11. Identificación de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipográficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "✍️ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedro\"]\n",
    "}\n",
    "\n",
    "import difflib ## Módulo que permite encontrar diferencias/similitudes en secuencias.\n",
    "\n",
    "nombres = [i.replace(' ','') for i in diccionario_once[\"cliente\"]] ## List comprenhension que limpiar espacios en blanco\n",
    "\n",
    "sospechosos_duplicados = {} ## Diccionario para almacenar duplicados sospechosos\n",
    "\n",
    "for nombre in nombres: ## Iteramos en a lista anterior.\n",
    "    ## .get_close_matches() Retorna las palabras similares a la primera que encuentre.\n",
    "    similares = difflib.get_close_matches(word=nombre,possibilities=nombres,n=3,cutoff=0.8)\n",
    "    ## word: Palabra a encontrar\n",
    "    ## possibilities: Lista de palabras similares\n",
    "    ## n: Cantidad de coincidencias\n",
    "    ## cutoff: Valor de similitud (Como requerimos las que sean casi similar usamos un 0.8=80%)\n",
    "    sospechosos_duplicados[nombre] = similares ## Almacenamos en el diccionario\n",
    "sospechosos_duplicados ## Imprimimos el diccionario\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. Validación y limpieza de rangos válidos\n",
    "\n",
    "🗃️ Dataset: Titanic\n",
    "🗒️ Enunciado: Valida que la columna \"age\" esté en un rango lógico (0 a 100 años).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "✍️ Resultado esperado: columna age sin valores inválidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_doce = sns.load_dataset(\"titanic\")\n",
    "# df_doce.shape[0] ## ➡️ Cantidad de datos originales: 891\n",
    "df_doce_clean = df_doce.query('age>0 and age<100') ## ✅ Filtramos la información y almacenamos en un nuevo dataset.\n",
    "df_doce_clean.shape[0] ## ➡️ Cantidad de datos originales: 714\n",
    "# df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. Normalización de categorías inconsistentes\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Detecta y unifica las categorías inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, corrección de errores).\n",
    "✍️ Resultado esperado: Dataset con categorías únicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "\n",
    "categorias_validas = [\"Lima\"] ## ➡️ Palabras válidas para su estandarización\n",
    "\n",
    "def normalizar_ciudad(valor):\n",
    "    match = difflib.get_close_matches(valor, categorias_validas, n=1, cutoff=0.6)\n",
    "    return match[0] if match else valor ## Verifica que sí la palabra ingresada\n",
    "                                        ## tiene una palabra válida similar, entonces\n",
    "                                        ## retorna esa palabra similar en una lista (accedemos a su posición).\n",
    "df_trece = pd.DataFrame(diccionario_trece)\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].str.title()\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].apply(normalizar_ciudad)\n",
    "df_trece.head()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f804d90",
   "metadata": {},
   "source": [
    "###### POLARS 🐻‍❄️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. Winsorización de outliers\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Aplica winsorización al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "✍️ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "💡 La winsorinización permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 11, 75, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pl.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head(7)\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## ➡️ Límite 5% inferior: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## ➡️ Límite 5% superior: 170.0 \n",
    "# limite_5_pct_superior\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"nota\")<limite_5_pct_inferior\n",
    "    ).then(pl.lit(limite_5_pct_inferior))\n",
    "    .when(\n",
    "        pl.col(\"nota\")>limite_5_pct_superior\n",
    "    ).then(pl.lit(limite_5_pct_superior))\n",
    "    .otherwise(pl.col(\"nota\")).alias(\"nota_estandarizada\")\n",
    ")\n",
    "df_nueve.head(7)\n",
    "\n",
    "\"\"\"\n",
    "10. Detección de inconsistencias de tipado en columnas\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Identifica las filas con tipos incorrectos  y conviértelas al tipo correcto.\n",
    "✍️ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pl.DataFrame(diccionario_diez,schema={\"ID\":pl.Object,\"Venta\":pl.Object})\n",
    "\"\"\"💡 En este caso definimos las columnas al tipo Object(permite datos de diversos tipos)\n",
    "   para poder manejar el error de inconsistencia de datos en las columnas.\"\"\"\n",
    "# df_diez.head()\n",
    "# df_diez_cast = df_diez.with_columns(\n",
    "#     pl.col(\"ID\").map_elements(lambda x:int(x),return_dtype=pl.Int64).alias(\"ID\"),\n",
    "#     pl.col(\"Venta\").map_elements(lambda x:float(x),return_dtype=pl.Float64).alias(\"Venta\")\n",
    "# )\n",
    "# df_diez_cast.head()\n",
    "\"\"\"\n",
    "💡 Para esta solución, Polars indica que map_elements es ineficiente al castear\n",
    "    estos datos debido a que evalua fila a fila la función. Sin embargo, son estas\n",
    "    casuísticas que nos permiten optar por estas soluciones.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "11. Identificación de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipográficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "✍️ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedroo\"]\n",
    "}\n",
    "import difflib\n",
    "nombres_estandarizados = [i.replace(' ','') for i in diccionario_once[\"cliente\"]]\n",
    "# nombres_estandarizados\n",
    "sospechosos_duplicados = {}\n",
    "for nombre in nombres_estandarizados:\n",
    "    sospechoso = difflib.get_close_matches(nombre,nombres_estandarizados,n=3,cutoff=0.8)\n",
    "    sospechosos_duplicados[nombre] = sospechoso\n",
    "sospechosos_duplicados \n",
    "\n",
    "\"\"\"\n",
    "12. Validación y limpieza de rangos válidos\n",
    "\n",
    "🗃️ Dataset: Titanic\n",
    "🗒️ Enunciado: Valida que la columna \"age\" esté en un rango lógico (0 a 100 años).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "✍️ Resultado esperado: columna age sin valores inválidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "df_doce = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce.shape[0] ## ➡️ Cantidad de datos iniciales: 891\n",
    "df_doce_clean = df_doce.filter(\n",
    "    ((pl.col(\"age\")>0) & (pl.col(\"age\")<100))\n",
    ")\n",
    "# df_doce_clean.head()\n",
    "df_doce_clean.shape[0] ## ➡️ Cantidad de datos filtrados: 714\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. Normalización de categorías inconsistentes\n",
    "\n",
    "🗃️ Dataset: Diccionario\n",
    "🗒️ Enunciado: Detecta y unifica las categorías inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, corrección de errores).\n",
    "✍️ Resultado esperado: Dataset con categorías únicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## ✔️ Solución\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "import difflib\n",
    "df_trece = pl.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "ciudades_estandarizadas = [\"Lima\"] # Lista de ciudades estandarizadas\n",
    "def estandarizar_ciudad(valor):\n",
    "    similar = difflib.get_close_matches(valor,ciudades_estandarizadas,n=1,cutoff=0.6)\n",
    "    return similar[0] if similar else valor\n",
    "df_trece = df_trece.with_columns(\n",
    "    pl.col(\"Ciudad\").str.to_titlecase().map_batches(estandarizar_ciudad).alias(\"Ciudad\")\n",
    ")\n",
    "df_trece.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
