{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e02fad",
   "metadata": {},
   "source": [
    "### ğŸ“˜ Talleres de IngenierÃ­a de Datos con Pandas y Polars ğŸ¼ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db83f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ğŸ‘¨â€ğŸ’» Autor: Brayan Neciosup  \n",
    "ğŸ“ Portafolio: [brayanneciosup](https://bryanneciosup626.wixsite.com/brayandataanalitics)  \n",
    "ğŸ”— LinkedIn: [linkedin.com/brayanneciosup](https://www.linkedin.com/in/brayan-rafael-neciosup-bola%C3%B1os-407a59246/)  \n",
    "ğŸ’» GitHub: [github.com/BrayanR03](https://github.com/BrayanR03)  \n",
    "ğŸ“š Serie: Fundamentos de Pandas y Polars   \n",
    "ğŸ““ Estos talleres constarÃ¡n de 3 niveles (BÃ¡sico-Intermedio-Avanzado)   \n",
    "ğŸ” AbarcarÃ¡ temas desde Fundamentos de Data Wrangling hacia Casos de Uso Avanzado   \n",
    "ğŸ“ Cada ejercicio presenta su enunciado, dataset, resultado esperado y soluciÃ³n.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cd600",
   "metadata": {},
   "source": [
    "#### FUNDAMENTOS DE DATA WRANGLING (MANIPULACIÃ“N DE DATOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90306f5b",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1a1a3",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51d9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. DetecciÃ³n de valores nulos en columnas principales\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Identifica cuÃ¡ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "âœï¸ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = sns.load_dataset(\"titanic\")\n",
    "# df_uno.head()\n",
    "# df_uno[[\"age\",\"embarked\",\"deck\"]].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: age(177) - embarked(2) - deck(688)\n",
    "\n",
    "\"\"\"\n",
    "2. EliminaciÃ³n de filas duplicadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Elimina las filas duplicadas y conserva solo la primera apariciÃ³n de cada registro.\n",
    "âœï¸ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"MarÃ­a\",\"Pedro\",\"Pedro\",\"SofÃ­a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pd.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "# df_dos.shape[0] ## â¡ï¸ Cantidad de datos: 7\n",
    "df_dos.drop_duplicates(subset=[\"id\",\"nombre\",\"edad\"],keep=\"first\",inplace=True)\n",
    "# df_dos.head()\n",
    "df_dos.shape[0] ## â¡ï¸ Cantidad de datos: 5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: PENGUINS\n",
    "ğŸ—’ï¸ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "âœï¸ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = sns.load_dataset(\"penguins\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 2\n",
    "media_bill_length_mm = float(np.mean(df_tres[\"bill_length_mm\"].dropna()).round(2))\n",
    "df_tres.fillna({\"bill_length_mm\":media_bill_length_mm},inplace=True)\n",
    "df_tres[\"bill_length_mm\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3644744",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f88ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "1. DetecciÃ³n de valores nulos en columnas principales\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Identifica cuÃ¡ntos valores faltantes hay en las columnas age, embarked y deck.\n",
    "âœï¸ Resultado esperado: un conteo por columna con la cantidad de valores nulos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_uno.head()\n",
    "df_uno.null_count()\n",
    "\n",
    "\"\"\"\n",
    "2. EliminaciÃ³n de filas duplicadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Elimina las filas duplicadas y conserva solo la primera apariciÃ³n de cada registro.\n",
    "âœï¸ Resultado esperado: un DataFrame sin filas repetidas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "dict_data = {\n",
    " \"id\": [1,2,2,3,4,4,5],\n",
    " \"nombre\": [\"Ana\",\"Luis\",\"Luis\",\"MarÃ­a\",\"Pedro\",\"Pedro\",\"SofÃ­a\"],\n",
    " \"edad\": [23,30,30,22,40,40,29]\n",
    "}\n",
    "df_dos = pl.DataFrame(dict_data)\n",
    "# df_dos.head()\n",
    "df_dos = df_dos.unique(keep=\"first\")\n",
    "df_dos.head()\n",
    "\n",
    "\"\"\"\n",
    "3. Reemplazo simple de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: PENGUINS\n",
    "ğŸ—’ï¸ Enunciado: Reemplaza los valores nulos en la columna bill_length_mm con la media de esa misma columna.\n",
    "âœï¸ Resultado esperado: columna sin valores nulos en bill_length_mm.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_tres = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_tres.head()\n",
    "# df_tres[\"bill_length_mm\"].null_count() ## â¡ï¸ Cantidad Nulos: 2\n",
    "media_bill_length_mm = df_tres[\"bill_length_mm\"].mean().__round__(2)\n",
    "media_bill_length_mm\n",
    "df_tres = df_tres.with_columns(\n",
    "    pl.col(\"bill_length_mm\").fill_null(media_bill_length_mm).alias(\"bill_length_mm\")\n",
    ")\n",
    "# df_tres.head()\n",
    "df_tres[\"bill_length_mm\"].null_count() ## â¡ï¸ Cantidad Nulos: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87889e12",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f9c68",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "4. ImputaciÃ³n condicional de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "âœï¸ Resultado esperado: columna age sin valores nulos, imputada segÃºn clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = sns.load_dataset(\"titanic\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "edad_promedio_por_clase = df_cuatro.groupby(\"pclass\",as_index=False,observed=True)[\"age\"].mean().round(2)\n",
    "edad_promedio_por_clase\n",
    "# df_cuatro[\"age\"].isnull().sum() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "\n",
    "df_cuatro_clean = df_cuatro.copy()\n",
    "df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1] = df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==1')[\"age\"][0])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==2] = df_cuatro[df_cuatro[\"pclass\"]==2].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query('pclass==2')[\"age\"][1])\n",
    "})\n",
    "df_cuatro[df_cuatro[\"pclass\"]==3] = df_cuatro[df_cuatro[\"pclass\"]==3].fillna({\n",
    "    \"age\":float(edad_promedio_por_clase.query(\"pclass==3\")[\"age\"][2])\n",
    "})\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==1].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==2].isnull().sum() ## Cantidad: 0\n",
    "# df_cuatro_clean[df_cuatro_clean[\"pclass\"]==3].isnull().sum() ## Cantidad: 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. DetecciÃ³n de outliers usando IQR\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica los valores de ventas que son outliers segÃºn el rango intercuartÃ­lico (IQR).\n",
    "âœï¸ Resultado esperado: listado de los productos que presentan valores anÃ³malos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "# df_cinco.describe()\n",
    "q1_ventas = float(np.quantile(df_cinco[\"ventas\"],0.25))\n",
    "q3_ventas = float(np.quantile(df_cinco[\"ventas\"],0.75))\n",
    "iqr_ventas = q3_ventas-q1_ventas\n",
    "lower_bound_ventas = q1_ventas - 1.5 * iqr_ventas\n",
    "upper_bound_ventas = q3_ventas + 1.5 * iqr_ventas\n",
    "df_cinco_outliers = df_cinco[(df_cinco[\"ventas\"]<lower_bound_ventas) | (df_cinco[\"ventas\"]>upper_bound_ventas)]\n",
    "# df_cinco_outliers.head()\n",
    "df_cinco_outliers.shape[0] ## CANTIDAD DE OUTLIERS EN COLUMNA VENTAS: 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. EliminaciÃ³n selectiva de duplicados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "âœï¸ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = sns.load_dataset(\"diamonds\")\n",
    "df_seis.head()\n",
    "# df_seis.shape[0] ## 53940 DATOS\n",
    "# df_seis[\"carat\"].duplicated().sum() ## 53 667 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis[\"price\"].duplicated().sum() ## 42 338 DATOS DUPLICADOS EN ESTA COLUMNA\n",
    "# df_seis.drop_duplicates(subset=[\"carat\",\"price\"],inplace=True)\n",
    "# df_seis.shape[0] ## 28988 DATOS DESPUES DE REMOVER DUPLICADOS\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaciÃ³n\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolaciÃ³n lineal.\n",
    "âœï¸ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_siete = {\n",
    "    \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    "    \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pd.DataFrame(diccionario_siete)\n",
    "df_siete_interpolado = df_siete.copy()\n",
    "df_siete_interpolado = df_siete_interpolado.interpolate(method=\"linear\")\n",
    "df_siete_interpolado.head()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Penguins\n",
    "ğŸ—’ï¸ Enunciado: Calcula el nÃºmero de registros que tienen valores nulos simultÃ¡neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "âœï¸ Resultado esperado: un nÃºmero entero que indique cuÃ¡ntos registros cumplen esta condiciÃ³n.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = sns.load_dataset(\"penguins\")\n",
    "# df_ocho.head()\n",
    "df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].isnull().sum() ## â¬…ï¸ Cantidad de datos nulos en ambas columnas (bill_length_mm:2 - bill_depth_mm: 2).\n",
    "df_ocho_nulos = df_ocho[(df_ocho[\"bill_length_mm\"].isnull()==True) & (df_ocho[\"bill_depth_mm\"].isnull()==True)]\n",
    "df_ocho_nulos.shape[0] ## â¬…ï¸ Cantidad de valores nulos simultaneos en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo ğŸ‘.\n",
    "df_example = pd.DataFrame(data=[[1,None],[None,2],[None,None]],columns=[\"A\",\"B\"])\n",
    "# df_example.head() ## â¬…ï¸ Como podremos observar hay un match en dos registros de la fila A y B que son nulos.\n",
    "cantidad_nulos = df_example[(df_example[\"A\"].isnull()==True) & (df_example[\"B\"].isnull()==True)]\n",
    "cantidad_nulos.shape[0] ## â¬…ï¸ Cantidad de valores nulos simultaneos en ambas columnas: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d7d0b",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bbcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "4. ImputaciÃ³n condicional de valores faltantes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: TITANIC\n",
    "ğŸ—’ï¸ Enunciado: Completa los valores faltantes de age con la edad promedio por clase (pclass).\n",
    "âœï¸ Resultado esperado: columna age sin valores nulos, imputada segÃºn clase de pasajero.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_cuatro = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_cuatro.head()\n",
    "# df_cuatro[\"age\"].null_count() ## â¡ï¸ Cantidad de datos nulos: 177\n",
    "def llenar_nulos_pclass_edad(dataframe):\n",
    "    df = dataframe\n",
    "    edad_promedio_por_clase = df_cuatro.group_by(\"pclass\").agg(\n",
    "        pl.col(\"age\").drop_nulls().mean().round(2).alias(\"avg_age_pclass\")\n",
    "    )\n",
    "    \n",
    "    for pclass,media in edad_promedio_por_clase.iter_rows():\n",
    "        df = df.with_columns(\n",
    "            pl.when(\n",
    "                (pl.col(\"pclass\")==pclass) & (pl.col(\"age\").is_null())\n",
    "            ).then(pl.lit(media))\n",
    "            .otherwise(pl.col(\"age\")).alias(\"age\")\n",
    "        )\n",
    "    return df\n",
    "df_cuatro_clean = llenar_nulos_pclass_edad(df_cuatro)\n",
    "df_cuatro_clean[[\"age\"]].null_count() ## â¡ï¸ Cantidad de datos nulos: 0 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "5. DetecciÃ³n de outliers usando IQR\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica los valores de ventas que son outliers segÃºn el rango intercuartÃ­lico (IQR).\n",
    "âœï¸ Resultado esperado: listado de los productos que presentan valores anÃ³malos.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    " \"producto\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    " \"ventas\": [120, 130, 115, 1000, 140, 135]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "q1_ventas = df_cinco[\"ventas\"].quantile(0.25)\n",
    "q1_ventas\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "6. EliminaciÃ³n selectiva de duplicados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: En el dataset de diamantes, elimina duplicados basados solo en las columnas carat y price.\n",
    "âœï¸ Resultado esperado: DataFrame sin duplicados en esas dos columnas, pero manteniendo el resto de filas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_seis.head()\n",
    "df_seis = df_seis.unique(subset=[\"carat\",\"price\"],keep=\"first\")\n",
    "df_seis.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "7. Relleno de valores faltantes con interpolaciÃ³n\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Rellena los valores nulos de la columna temperatura mediante interpolaciÃ³n lineal.\n",
    "âœï¸ Resultado esperado: columna completa sin valores nulos, con estimaciones suaves.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_siete  = {\n",
    " \"fecha\": pd.date_range(\"2024-01-01\", periods=10),\n",
    " \"temperatura\": [21,22,None,24,25,None,None,28,29,30]\n",
    "}\n",
    "df_siete = pl.DataFrame(diccionario_siete)\n",
    "df_siete = df_siete.select(pl.col(\"fecha\"),pl.col(\"temperatura\").interpolate().alias(\"temperatura\"))\n",
    "df_siete.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. Conteo de valores faltantes combinados\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Penguins\n",
    "ğŸ—’ï¸ Enunciado: Calcula el nÃºmero de registros que tienen valores nulos simultÃ¡neamente\n",
    "    en las columnas bill_length_mm y bill_depth_mm.\n",
    "âœï¸ Resultado esperado: un nÃºmero entero que indique cuÃ¡ntos registros cumplen esta condiciÃ³n.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_ocho = pl.read_csv(\"../datasets/penguins.csv\",separator=\",\")\n",
    "# df_ocho.head()\n",
    "# df_ocho[[\"bill_length_mm\",\"bill_depth_mm\"]].null_count() ## â¡ï¸ Cantidad de Valores Nulos\n",
    "nulos_consecutivos = df_ocho.filter(\n",
    "    (pl.col(\"bill_length_mm\").is_null() & pl.col(\"bill_depth_mm\").is_null())\n",
    ")\n",
    "# nulos_consecutivos.head() ## â¬…ï¸ Verificamos nulos simulares en ambas columnas\n",
    "# nulos_consecutivos.shape[0] ## â¬…ï¸ Cantidad de nulos simulares en ambas columnas: 2\n",
    "\n",
    "#--- En caso no haya entendido el concepto, te dejo este otro ejemplo ğŸ‘.\n",
    "df_example = pl.DataFrame(data=[[1,None],[None,2],[None,None]],schema=[\"A\",\"B\"],orient=\"row\")\n",
    "# df_example.head() ## â¬…ï¸ Verificamos que existen nulos en columnas similares\n",
    "nulos_consecutivos_2 = df_example.filter(\n",
    "    (pl.col(\"A\").is_null() & pl.col(\"B\").is_null())\n",
    ")\n",
    "# nulos_consecutivos_2.head() ## â¬…ï¸ Verificamos nulos simulares en ambas columnas\n",
    "nulos_consecutivos_2.shape[0] ## â¬…ï¸ Cantidad de nulos simulares en ambas columnas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db608e",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a022a",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a91a88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ciudad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lima</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ciudad\n",
       "0   Lima\n",
       "1   Lima\n",
       "2   Lima\n",
       "3   Lima"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. WinsorizaciÃ³n de outliers\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica winsorizaciÃ³n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "âœï¸ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "ğŸ’¡ La winsorinizaciÃ³n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 12, 15, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pd.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## â¡ï¸ Quartil 5%: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## â¡ï¸ Quartil 95%: 170.0\n",
    "# limite_5_pct_superior\n",
    "condiciones_valores = [\n",
    "    (df_nueve[\"nota\"]<limite_5_pct_inferior), ## Condicina que, si el valor es menor al lÃ­mite 5% inferior\n",
    "    (df_nueve[\"nota\"]>limite_5_pct_superior)  ## Condicina que, si el valor es mayor al lÃ­mite 95% superior\n",
    "]\n",
    "values = np.array([limite_5_pct_inferior,limite_5_pct_superior]) ## Se establece los valores de las condiciones\n",
    "df_nueve_winzorizacion = df_nueve.copy()\n",
    "df_nueve_winzorizacion[\"nota_winsorizacion\"] = np.select(condiciones_valores,values,default=df_nueve[\"nota\"])\n",
    "df_nueve_winzorizacion.head(7)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "10. DetecciÃ³n de inconsistencias de tipado en columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica las filas con tipos incorrectos  y conviÃ©rtelas al tipo correcto.\n",
    "âœï¸ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "df_diez[\"ID\"] = df_diez[\"ID\"].astype(dtype=\"int\") ## Casteamos el tipo de dato a int\n",
    "df_diez[\"Venta\"] = df_diez[\"Venta\"].astype(dtype=\"float\") ## Casteamos el tipo de dato a float\n",
    "# df_diez.head()\n",
    "df_diez.dtypes ##  âœ… Verificamos los tipos de dato correctamente casteados.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "11. IdentificaciÃ³n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogrÃ¡ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "âœï¸ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedro\"]\n",
    "}\n",
    "\n",
    "import difflib ## MÃ³dulo que permite encontrar diferencias/similitudes en secuencias.\n",
    "\n",
    "nombres = [i.replace(' ','') for i in diccionario_once[\"cliente\"]] ## List comprenhension que limpiar espacios en blanco\n",
    "\n",
    "sospechosos_duplicados = {} ## Diccionario para almacenar duplicados sospechosos\n",
    "\n",
    "for nombre in nombres: ## Iteramos en a lista anterior.\n",
    "    ## .get_close_matches() Retorna las palabras similares a la primera que encuentre.\n",
    "    similares = difflib.get_close_matches(word=nombre,possibilities=nombres,n=3,cutoff=0.8)\n",
    "    ## word: Palabra a encontrar\n",
    "    ## possibilities: Lista de palabras similares\n",
    "    ## n: Cantidad de coincidencias\n",
    "    ## cutoff: Valor de similitud (Como requerimos las que sean casi similar usamos un 0.8=80%)\n",
    "    sospechosos_duplicados[nombre] = similares ## Almacenamos en el diccionario\n",
    "sospechosos_duplicados ## Imprimimos el diccionario\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. ValidaciÃ³n y limpieza de rangos vÃ¡lidos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Valida que la columna \"age\" estÃ© en un rango lÃ³gico (0 a 100 aÃ±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "âœï¸ Resultado esperado: columna age sin valores invÃ¡lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = sns.load_dataset(\"titanic\")\n",
    "# df_doce.shape[0] ## â¡ï¸ Cantidad de datos originales: 891\n",
    "df_doce_clean = df_doce.query('age>0 and age<100') ## âœ… Filtramos la informaciÃ³n y almacenamos en un nuevo dataset.\n",
    "df_doce_clean.shape[0] ## â¡ï¸ Cantidad de datos originales: 714\n",
    "# df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n de categorÃ­as inconsistentes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta y unifica las categorÃ­as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcciÃ³n de errores).\n",
    "âœï¸ Resultado esperado: Dataset con categorÃ­as Ãºnicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "\n",
    "categorias_validas = [\"Lima\"] ## â¡ï¸ Palabras vÃ¡lidas para su estandarizaciÃ³n\n",
    "\n",
    "def normalizar_ciudad(valor):\n",
    "    match = difflib.get_close_matches(valor, categorias_validas, n=1, cutoff=0.6)\n",
    "    return match[0] if match else valor ## Verifica que sÃ­ la palabra ingresada\n",
    "                                        ## tiene una palabra vÃ¡lida similar, entonces\n",
    "                                        ## retorna esa palabra similar en una lista (accedemos a su posiciÃ³n).\n",
    "df_trece = pd.DataFrame(diccionario_trece)\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].str.title()\n",
    "df_trece[\"Ciudad\"] = df_trece[\"Ciudad\"].apply(normalizar_ciudad)\n",
    "df_trece.head()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f804d90",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "9. WinsorizaciÃ³n de outliers\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica winsorizaciÃ³n al 5% superior e inferior en la\n",
    "   columna nota para reducir el impacto de valores extremos.\n",
    "âœï¸ Resultado esperado: columna nota ajustada, sin eliminar registros.\n",
    "\n",
    "ğŸ’¡ La winsorinizaciÃ³n permite mejorar la integridad y confiabilidad de datos\n",
    "    evitando valores extremos a los limites de quartil que tiene cada columna.\n",
    "    Por ejemplo: valores mayores a 10, se establecen como 10 y valores\n",
    "    menores a 5, se establecen como 5.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    " \"alumno\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"],\n",
    " \"nota\": [10, 11, 75, 100, 11, 13, 200]\n",
    "}\n",
    "\n",
    "df_nueve = pl.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head(7)\n",
    "limite_5_pct_inferior = float(np.quantile(df_nueve[\"nota\"],0.05)) ## â¡ï¸ LÃ­mite 5% inferior: 10.3\n",
    "# limite_5_pct_inferior\n",
    "limite_5_pct_superior = float(np.quantile(df_nueve[\"nota\"],0.95).round(2)) ## â¡ï¸ LÃ­mite 5% superior: 170.0 \n",
    "# limite_5_pct_superior\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"nota\")<limite_5_pct_inferior\n",
    "    ).then(pl.lit(limite_5_pct_inferior))\n",
    "    .when(\n",
    "        pl.col(\"nota\")>limite_5_pct_superior\n",
    "    ).then(pl.lit(limite_5_pct_superior))\n",
    "    .otherwise(pl.col(\"nota\")).alias(\"nota_estandarizada\")\n",
    ")\n",
    "df_nueve.head(7)\n",
    "\n",
    "\"\"\"\n",
    "10. DetecciÃ³n de inconsistencias de tipado en columnas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Identifica las filas con tipos incorrectos  y conviÃ©rtelas al tipo correcto.\n",
    "âœï¸ Resultado esperado: Un dataset con tipos de datos correcto en cada columna\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_diez = {\n",
    "    \"ID\":[1,\"2\",3,\"004\",\"5\"],\n",
    "    \"Venta\":[\"12254\",1450,1200.00,\"300\",120.00]\n",
    "}\n",
    "df_diez = pl.DataFrame(diccionario_diez,schema={\"ID\":pl.Object,\"Venta\":pl.Object})\n",
    "\"\"\"ğŸ’¡ En este caso definimos las columnas al tipo Object(permite datos de diversos tipos)\n",
    "   para poder manejar el error de inconsistencia de datos en las columnas.\"\"\"\n",
    "# df_diez.head()\n",
    "# df_diez_cast = df_diez.with_columns(\n",
    "#     pl.col(\"ID\").map_elements(lambda x:int(x),return_dtype=pl.Int64).alias(\"ID\"),\n",
    "#     pl.col(\"Venta\").map_elements(lambda x:float(x),return_dtype=pl.Float64).alias(\"Venta\")\n",
    "# )\n",
    "# df_diez_cast.head()\n",
    "\"\"\"\n",
    "ğŸ’¡ Para esta soluciÃ³n, Polars indica que map_elements es ineficiente al castear\n",
    "    estos datos debido a que evalua fila a fila la funciÃ³n. Sin embargo, son estas\n",
    "    casuÃ­sticas que nos permiten optar por estas soluciones.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "11. IdentificaciÃ³n de duplicados aproximados (fuzzy matching)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta nombres de clientes que parecen duplicados\n",
    "    por errores tipogrÃ¡ficos (ejemplo: \"Luis\" vs \"luiz\").\n",
    "âœï¸ Resultado esperado: listado de pares de valores sospechosos de ser duplicados.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_once = {\n",
    " \"cliente\": [\"Ana\", \"Ana \", \"Luis\", \"Luz\", \"luiz\", \"Pedro\", \"pedroo\"]\n",
    "}\n",
    "import difflib\n",
    "nombres_estandarizados = [i.replace(' ','') for i in diccionario_once[\"cliente\"]]\n",
    "# nombres_estandarizados\n",
    "sospechosos_duplicados = {}\n",
    "for nombre in nombres_estandarizados:\n",
    "    sospechoso = difflib.get_close_matches(nombre,nombres_estandarizados,n=3,cutoff=0.8)\n",
    "    sospechosos_duplicados[nombre] = sospechoso\n",
    "sospechosos_duplicados \n",
    "\n",
    "\"\"\"\n",
    "12. ValidaciÃ³n y limpieza de rangos vÃ¡lidos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Valida que la columna \"age\" estÃ© en un rango lÃ³gico (0 a 100 aÃ±os).\n",
    "               Detecta y corrige/descarta valores fuera de rango.\n",
    "âœï¸ Resultado esperado: columna age sin valores invÃ¡lidos, garantizando integridad de negocio.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce.shape[0] ## â¡ï¸ Cantidad de datos iniciales: 891\n",
    "df_doce_clean = df_doce.filter(\n",
    "    ((pl.col(\"age\")>0) & (pl.col(\"age\")<100))\n",
    ")\n",
    "# df_doce_clean.head()\n",
    "df_doce_clean.shape[0] ## â¡ï¸ Cantidad de datos filtrados: 714\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n de categorÃ­as inconsistentes\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Detecta y unifica las categorÃ­as inconsistentes aplicando reglas \n",
    "              de limpieza (case folding, correcciÃ³n de errores).\n",
    "âœï¸ Resultado esperado: Dataset con categorÃ­as Ãºnicas y estandarizadas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"Ciudad\":[\"Lima\",\"lima\",\"LIMA\",\"Lma\",]\n",
    "}\n",
    "import difflib\n",
    "df_trece = pl.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "ciudades_estandarizadas = [\"Lima\"] # Lista de ciudades estandarizadas\n",
    "def estandarizar_ciudad(valor):\n",
    "    similar = difflib.get_close_matches(valor,ciudades_estandarizadas,n=1,cutoff=0.6)\n",
    "    return similar[0] if similar else valor\n",
    "df_trece = df_trece.with_columns(\n",
    "    pl.col(\"Ciudad\").str.to_titlecase().map_batches(estandarizar_ciudad).alias(\"Ciudad\")\n",
    ")\n",
    "df_trece.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787fb12",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING (INGENIERÃA DE CARACTERÃSTICAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337f29c",
   "metadata": {},
   "source": [
    "##### ğŸ¥‰ NIVEL BÃSICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba96807",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4115cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>longitud_comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excelente servicio</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muy caro</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aceptable</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           comentario  longitud_comentario\n",
       "0  Excelente servicio                   18\n",
       "1            Muy caro                    8\n",
       "2           Aceptable                    9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "1. Variables dummies simples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte la columna sex en variables dummies\n",
    "âœï¸ Resultado esperado: Dos columnas adicionales (sex_male, sex_female) con valores binarios 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = sns.load_dataset(\"titanic\")\n",
    "# df_uno.head()\n",
    "df_dummies_sex = pd.get_dummies(data=df_uno[\"sex\"],columns=[\"sex\"])\n",
    "df_uno_final = pd.concat([df_uno,df_dummies_sex],axis=1)\n",
    "df_uno_final\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"OneHot-Encoding\"\n",
    "##    la cuÃ¡l permite transformar datos en valores Ã³ptimos para modelos de Machine Learning (ML).\n",
    "\n",
    "\"\"\"\n",
    "2. Binning por intervalos fijos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa la columna total_bill en 3 intervalos: bajo(<10), medio(>=10 y <20), alto(>=20).\n",
    "âœï¸ Resultado esperado: Nueva columna total_bill_bin con categorÃ­as: \"Bajo\", \"Medio\", \"Alto\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = sns.load_dataset(\"tips\")\n",
    "# df_dos.head()\n",
    "condiciones = [\n",
    "    (df_dos[\"total_bill\"]<10),\n",
    "    (df_dos[\"total_bill\"]>=10) & (df_dos[\"total_bill\"]<20),\n",
    "    (df_dos[\"total_bill\"]>20)\n",
    "]\n",
    "valores = np.array([\"Bajo\",\"Medio\",\"Alto\"],dtype=object)\n",
    "df_dos[\"total_bill_bin\"] = np.select(condlist=condiciones,choicelist=valores,default=\"F\")\n",
    "df_dos.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. NormalizaciÃ³n min-max\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Normaliza la columna ventas entre 0 y 1 usando min-max.\n",
    "âœï¸ Resultado esperado: Nueva columna ventas_norm con valores escalados entre 0 y 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_tres = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"ventas\":[100,300,500]\n",
    "}\n",
    "df_tres = pd.DataFrame(diccionario_tres)\n",
    "# df_tres.head()\n",
    "df_tres[\"ventas_normalizada\"] = ((df_tres[\"ventas\"]-df_tres[\"ventas\"].min())/(df_tres[\"ventas\"].max()-df_tres[\"ventas\"].min()))\n",
    "df_tres.head()\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"NormalizaciÃ³n Min-Max\"\n",
    "##    la cuÃ¡l permite establecer en un rango de 0 y 1 valores que permitan a modelos de ML\n",
    "##    aprener los patrones de los datos, pero con variables en escalas comparables mejorando su rendimiento.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ExtracciÃ³n de aÃ±o y mes de fechas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Enunciado: Extrae el aÃ±o y mes de la columna fecha.\n",
    "âœï¸ Resultado esperado: Dos nuevas columnas: aÃ±o y mes.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12\",\"2022-01-20\",\"2023-07-15\"]\n",
    "}\n",
    "df_cuatro = pd.DataFrame(data=diccionario_cuatro)\n",
    "# df_cuatro.head()\n",
    "df_cuatro[\"fecha\"] = pd.to_datetime(df_cuatro[\"fecha\"]) ## â¬…ï¸ Convertir la columna fecha (str) a tipo datetime\n",
    "df_cuatro[\"aÃ±o\"] = df_cuatro[\"fecha\"].dt.year ## â¬…ï¸ Extraer el AÃ±o\n",
    "df_cuatro[\"mes\"] = df_cuatro[\"fecha\"].dt.month ## â¬…ï¸ Extraer el Mes\n",
    "df_cuatro.head()\n",
    "\n",
    "\"\"\"\n",
    "5. Longitud de cadenas de texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con la longitud de caracteres de cada comentario.\n",
    "âœï¸ Resultado esperado: Columna longitud con nÃºmero de caracteres por fila.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"comentario\":[\"Excelente servicio\",\"Muy caro\",\"Aceptable\"]\n",
    "}\n",
    "df_cinco = pd.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "df_cinco[\"longitud_comentario\"] = df_cinco[\"comentario\"].str.len()\n",
    "df_cinco.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be1caa",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10032fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>longitud_caracter</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Excelente servicio&quot;</td><td>18</td></tr><tr><td>&quot;Muy caro&quot;</td><td>8</td></tr><tr><td>&quot;Aceptable&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario         â”† longitud_caracter â”‚\n",
       "â”‚ ---                â”† ---               â”‚\n",
       "â”‚ str                â”† u32               â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Excelente servicio â”† 18                â”‚\n",
       "â”‚ Muy caro           â”† 8                 â”‚\n",
       "â”‚ Aceptable          â”† 9                 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "1. Variables dummies simples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte la columna sex en variables dummies\n",
    "âœï¸ Resultado esperado: Dos columnas adicionales (sex_male, sex_female) con valores binarios 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_uno = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_uno.head()\n",
    "\n",
    "df_sex_dummies = df_uno.to_dummies(columns=[\"sex\"])\n",
    "df_uno_final = pl.concat([df_uno,df_sex_dummies[[\"sex_male\",\"sex_female\"]]],how=\"horizontal\")\n",
    "df_uno_final\n",
    "## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"OneHot-Encoding\"\n",
    "##    la cuÃ¡l permite transformar datos en valores Ã³ptimos para modelos de Machine Learning (ML).\n",
    "\n",
    "\"\"\"\n",
    "2. Binning por intervalos fijos\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Agrupa la columna total_bill en 3 intervalos: bajo(<10), medio(>=10 y <20), alto(>=20).\n",
    "âœï¸ Resultado esperado: Nueva columna total_bill_bin con categorÃ­as: \"Bajo\", \"Medio\", \"Alto\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_dos = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_dos.head()\n",
    "df_dos = df_dos.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"total_bill\")<10\n",
    "    ).then(\n",
    "        pl.lit(\"Bajo\")\n",
    "    ).when(\n",
    "        (pl.col(\"total_bill\")>=10) & (pl.col(\"total_bill\")<20)\n",
    "    ).then(pl.lit(\"Medio\"))\n",
    "    .otherwise(pl.lit(\"Alto\"))\n",
    ")\n",
    "df_dos.head()\n",
    "\n",
    "\"\"\"\n",
    "3. NormalizaciÃ³n min-max\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Normaliza la columna ventas entre 0 y 1 usando min-max.\n",
    "âœï¸ Resultado esperado: Nueva columna ventas_norm con valores escalados entre 0 y 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_tres = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\"],\n",
    "    \"ventas\":[100,300,500]\n",
    "}\n",
    "df_tres = pl.DataFrame(diccionario_tres)\n",
    "# df_tres.head()\n",
    "df_tres = df_tres.with_columns(\n",
    "    ((pl.col(\"ventas\")-pl.col(\"ventas\").min())/(pl.col(\"ventas\").max()-pl.col(\"ventas\").min())).alias(\"ventas_normalizada\")\n",
    ")\n",
    "df_tres.head()\n",
    "\n",
    "# ## ğŸ’¡ Esta es una tÃ©ncica de pre-procesamiento de datos llamada \"NormalizaciÃ³n Min-Max\"\n",
    "# ##    la cuÃ¡l permite establecer en un rango de 0 y 1 valores que permitan a modelos de ML\n",
    "# ##    aprener los patrones de los datos, pero con variables en escalas comparables mejorando su rendimiento.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. ExtracciÃ³n de aÃ±o y mes de fechas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Enunciado: Extrae el aÃ±o y mes de la columna fecha.\n",
    "âœï¸ Resultado esperado: Dos nuevas columnas: aÃ±o y mes.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cuatro = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12\",\"2022-01-20\",\"2023-07-15\"]\n",
    "}\n",
    "df_cuatro = pl.DataFrame(diccionario_cuatro)\n",
    "# df_cuatro.head()\n",
    "df_cuatro = df_cuatro.with_columns(\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).dt.year().alias(\"AÃ±o\"),\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).dt.month().alias(\"Mes\")\n",
    ")\n",
    "df_cuatro.head()\n",
    "\n",
    "\"\"\"\n",
    "5. Longitud de cadenas de texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con la longitud de caracteres de cada comentario.\n",
    "âœï¸ Resultado esperado: Columna longitud con nÃºmero de caracteres por fila.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_cinco = {\n",
    "    \"comentario\":[\"Excelente servicio\",\"Muy caro\",\"Aceptable\"]\n",
    "}\n",
    "df_cinco = pl.DataFrame(diccionario_cinco)\n",
    "# df_cinco.head()\n",
    "df_cinco = df_cinco.with_columns(\n",
    "    pl.col(\"comentario\").str.len_chars().alias(\"longitud_caracter\")\n",
    ")\n",
    "df_cinco.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2712f8b",
   "metadata": {},
   "source": [
    "##### ğŸ¥ˆ NIVEL INTERMEDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2adf4da",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce7ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>num_palabras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me gusta el servicio</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precio alto pero bueno</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No lo recomiendo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               comentario  num_palabras\n",
       "0    Me gusta el servicio             4\n",
       "1  Precio alto pero bueno             4\n",
       "2        No lo recomiendo             3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "6. Variables dummies mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte embarked en variables dummies.\n",
    "âœï¸ Resultado esperado: Nuevas columnas (embarked_C, embarked_Q, embarked_S) con valores 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = sns.load_dataset(\"titanic\")\n",
    "# df_seis.head()\n",
    "df_seis_embarked = pd.get_dummies(data=df_seis[\"embarked\"],prefix=\"embarked\")\n",
    "# df_seis_embarked.head()\n",
    "df_seis_final = pd.concat([df_seis,df_seis_embarked],axis=1)\n",
    "df_seis_final.head()\n",
    "\n",
    "\"\"\"\n",
    "7. Binning por cuantiles\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Divide la columna price en 4 categorÃ­as segÃºn sus cuartiles.\n",
    "âœï¸ Resultado esperado: Nueva columna price_bin con categorÃ­as Q1, Q2, Q3, Q4.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = sns.load_dataset(\"diamonds\")\n",
    "# df_siete.head()\n",
    "df_siete[\"price_bin\"] = pd.qcut(df_siete[\"price\"],q=4,labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"])\n",
    "df_siete.head()\n",
    "## ğŸ’¡ La divisiÃ³n de una columna en quantiles permite mantener la relaciÃ³n entre\n",
    "##    los datos originales y su clasificaciÃ³n por quantiles.\n",
    "\n",
    "\"\"\"\n",
    "8. EstandarizaciÃ³n (Z-score)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el Z-score de las notas.\n",
    "âœï¸ Resultado esperado: Nueva columna notas_zscore con valores centrados en media 0 y desviaciÃ³n estÃ¡ndar 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "lista_ocho = [\n",
    "    12, 14, 13, 15, 16, 11, 14, 13, 15, 12,  # Notas normales\n",
    "    14, 16, 13, 15, 14, 12, 13, 16, 15, 14,  # MÃ¡s notas normales\n",
    "    3,   # Nota muy baja (Outlier)\n",
    "    19   # Nota muy lta (Outlier)\n",
    "]\n",
    "\n",
    "df_ocho = pd.DataFrame({\n",
    "    \"Estudiante\": [f\"Estudiante {i+1}\" for i in range(len(lista_ocho))],\n",
    "    \"Nota\":lista_ocho\n",
    "})\n",
    "# df_ocho.head()\n",
    "\n",
    "## âœ… Calcular Z-SCORE\n",
    "##---- Importamos: from scipy import stats\n",
    "from scipy import stats\n",
    "\n",
    "df_ocho[\"notas_zscore\"] = stats.zscore(df_ocho[\"Nota\"])\n",
    "df_ocho.head()\n",
    "## ğŸ’¡ La estandarizaciÃ³n con z-score es una transformaciÃ³n estÃ¡ndar \n",
    "##    en ciencia de datos que \"normaliza\" los datos sin cambiar su distribuciÃ³n, solo su escala.\n",
    "\n",
    "\"\"\"\n",
    "9. DÃ­a de la semana desde fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae el dÃ­a de la semana de cada fecha.\n",
    "âœï¸ Resultado esperado: Columna dia_semana con valores tipo: \"Viernes\", \"SÃ¡bado\", \"Domingo\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2023-05-12\",\"2023-05-13\",\"2023-05-14\"]\n",
    "}\n",
    "df_nueve = pd.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "df_nueve[\"fecha\"] = pd.to_datetime(df_nueve[\"fecha\"])\n",
    "# df_nueve.head()\n",
    "df_nueve[\"dia_semana\"] = df_nueve[\"fecha\"].dt.day_name(locale=\"\")\n",
    "df_nueve.head()\n",
    "\n",
    "\"\"\"\n",
    "10. Conteo de palabras en texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras por comentario.\n",
    "âœï¸ Resultado esperado: Columna num_palabras con valores 4, 4, 3.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_diez = {\n",
    "    \"comentario\":[\"Me gusta el servicio\",\"Precio alto pero bueno\",\"No lo recomiendo\"]\n",
    "}\n",
    "\n",
    "df_diez = pd.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez[\"num_palabras\"] = df_diez[\"comentario\"].apply(lambda x:len(x.split(\" \")))\n",
    "df_diez.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518f057",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a65b845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>s</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Me gusta el servicio&quot;</td><td>4</td></tr><tr><td>&quot;Precio alto pero bueno&quot;</td><td>4</td></tr><tr><td>&quot;No lo recomiendo&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario             â”† s   â”‚\n",
       "â”‚ ---                    â”† --- â”‚\n",
       "â”‚ str                    â”† i64 â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
       "â”‚ Me gusta el servicio   â”† 4   â”‚\n",
       "â”‚ Precio alto pero bueno â”† 4   â”‚\n",
       "â”‚ No lo recomiendo       â”† 3   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "6. Variables dummies mÃºltiples\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Convierte embarked en variables dummies.\n",
    "âœï¸ Resultado esperado: Nuevas columnas (embarked_C, embarked_Q, embarked_S) con valores 0/1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_seis = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_seis.head()\n",
    "df_seis_dummies = df_seis.select(pl.col(\"embarked\").drop_nulls()).to_dummies(columns=[\"embarked\"])\n",
    "# df_seis_dummies.head()\n",
    "df_seis_final = pl.concat([df_seis,df_seis_dummies],how=\"horizontal\")\n",
    "df_seis_final.head()\n",
    "\n",
    "\"\"\"\n",
    "7. Binning por cuantiles\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diamonds\n",
    "ğŸ—’ï¸ Enunciado: Divide la columna price en 4 categorÃ­as segÃºn sus cuartiles.\n",
    "âœï¸ Resultado esperado: Nueva columna price_bin con categorÃ­as Q1, Q2, Q3, Q4.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_siete = pl.read_csv(\"../datasets/diamonds.csv\",separator=\",\")\n",
    "# df_siete.head()\n",
    "df_siete = df_siete.with_columns(\n",
    "    pl.col(\"price\").qcut(quantiles=4,labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"]).alias(\"price_bin\")\n",
    ")\n",
    "df_siete.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. EstandarizaciÃ³n (Z-score)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Calcula el Z-score de las notas.\n",
    "âœï¸ Resultado esperado: Nueva columna notas_zscore con valores centrados en media 0 y desviaciÃ³n estÃ¡ndar 1.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "lista_ocho = [\n",
    "    12, 14, 13, 15, 16, 11, 14, 13, 15, 12,  # Notas normales\n",
    "    14, 16, 13, 15, 14, 12, 13, 16, 15, 14,  # MÃ¡s notas normales\n",
    "    3,   # Nota muy baja (Outlier)\n",
    "    19   # Nota muy lta (Outlier)\n",
    "]\n",
    "\n",
    "df_ocho = pl.DataFrame({\n",
    "    \"Estudiante\": [f\"Estudiante {i+1}\" for i in range(len(lista_ocho))],\n",
    "    \"Nota\":lista_ocho\n",
    "})\n",
    "# df_ocho.head()\n",
    "\n",
    "## âœ… Calcular Z-SCORE\n",
    "##---- Importamos: from scipy import stats\n",
    "from scipy import stats\n",
    "### --- Extraemos la columna como serie Numpy\n",
    "nota = df_ocho[\"Nota\"].to_numpy()\n",
    "### --- Calculamos el z-score.\n",
    "serie_nota_zscore = stats.zscore(nota)\n",
    "df_ocho = df_ocho.with_columns(\n",
    "    pl.Series(\"nota_zscore\",serie_nota_zscore) ## Agregamos la serie como columna de Polars\n",
    ")\n",
    "df_ocho.head()\n",
    "\n",
    "\"\"\"\n",
    "9. DÃ­a de la semana desde fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae el dÃ­a de la semana de cada fecha.\n",
    "âœï¸ Resultado esperado: Columna dia_semana con valores tipo: \"Viernes\", \"SÃ¡bado\", \"Domingo\".\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_nueve = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2023-05-12\",\"2023-05-13\",\"2023-05-14\"]\n",
    "}\n",
    "df_nueve = pl.DataFrame(diccionario_nueve)\n",
    "# df_nueve.head()\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.col(\"fecha\").cast(dtype=pl.Date).alias(\"fecha\")\n",
    ")\n",
    "df_nueve = df_nueve.with_columns(\n",
    "    pl.col(\"fecha\").dt.to_string(format=\"%A\").alias(\"nombre_dia\")\n",
    ")\n",
    "df_nueve.head()\n",
    "## ğŸ’¡ Es importante conocer los diversos formatos de fechas.\n",
    "\n",
    "\"\"\"\n",
    "10. Conteo de palabras en texto\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras por comentario.\n",
    "âœï¸ Resultado esperado: Columna num_palabras con valores 4, 4, 3.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "\n",
    "diccionario_diez = {\n",
    "    \"comentario\":[\"Me gusta el servicio\",\"Precio alto pero bueno\",\"No lo recomiendo\"]\n",
    "}\n",
    "\n",
    "df_diez = pl.DataFrame(diccionario_diez)\n",
    "# df_diez.head()\n",
    "df_diez = df_diez.with_columns(\n",
    "    pl.col(\"comentario\").map_elements(lambda x:len(x.split(\" \")),return_dtype=pl.Int64).alias(\"s\")\n",
    ")\n",
    "df_diez.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026ded4",
   "metadata": {},
   "source": [
    "##### ğŸ¥‡ NIVEL AVANZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94a393",
   "metadata": {},
   "source": [
    "###### PANDAS ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "726b1c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>palabras_unicas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muy buen buen servicio</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Servicio aceptable aceptable</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No me gustÃ³ el servicio</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     comentario  palabras_unicas\n",
       "0        Muy buen buen servicio                3\n",
       "1  Servicio aceptable aceptable                2\n",
       "2       No me gustÃ³ el servicio                5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "11. Variables categÃ³ricas cruzadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Crea una nueva variable categÃ³rica que combine pclass y sex (ejemplo: \"1_female\", \"3_male\").\n",
    "âœï¸ Resultado esperado: Columna clase_sexo con las combinaciones Ãºnicas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = sns.load_dataset(\"titanic\")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Creamos la variable que permita las combinaciones de datos en ambas columnas.\n",
    "df_once[\"clase_sexo\"] = df_once[\"pclass\"].astype(str)+'_'+df_once[\"sex\"]\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Categorizamos las variables de esa columna.\n",
    "dummies_pclass_sex = pd.get_dummies(data=df_once[[\"clase_sexo\"]],prefix=\"\")\n",
    "# dummies_pclass_sex\n",
    "\n",
    "## -- âœ… Unificamos al dataframe original\n",
    "df_once_final = pd.concat([df_once,dummies_pclass_sex],axis=\"columns\")\n",
    "df_once_final.head()\n",
    "\n",
    "\"\"\"\n",
    "12. Binning desigual basado en reglas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Clasifica tip en categorÃ­as: \"Bajo\" (<2), \"Medio\" (2 - 5), \"Alto\" (>5).\n",
    "âœï¸ Resultado esperado: Nueva columna tip_categoria con esas 3 categorÃ­as.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = sns.load_dataset(\"tips\")\n",
    "# df_doce.head()\n",
    "condiciones_valores = [\n",
    "    (df_doce[\"tip\"]<2),\n",
    "    (df_doce[\"tip\"]>=2) & (df_doce[\"tip\"]<=5),\n",
    "    (df_doce[\"tip\"]>5)\n",
    "]\n",
    "values = np.array([\"Bajo\",\"Medio\",\"Alto\"],object)\n",
    "df_doce[\"tip_categoria\"] = np.select(condlist=condiciones_valores,choicelist=values,default=\"F\")\n",
    "df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n robusta\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica una normalizaciÃ³n robusta (usando mediana e IQR).\n",
    "âœï¸ Resultado esperado: Columna ventas_robust que reduzca la influencia de outliers.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\",\"D\"],\n",
    "    \"ventas\":[10,200,1000,10000]\n",
    "}\n",
    "df_trece = pd.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "\n",
    "##  âœ… Hallamos los Quartiles (columna ventas)\n",
    "q1_ventas = float(np.quantile(df_trece[\"ventas\"],0.25))\n",
    "# q1_ventas\n",
    "q3_ventas = float(np.quantile(df_trece[\"ventas\"],0.75))\n",
    "# q3_ventas\n",
    "\n",
    "## âœ… Hallamos IQR (Rango Interquartil - columnas ventas)\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "# iqr_ventas\n",
    "\n",
    "## âœ… Hallamos mediana (columna ventas)\n",
    "mediana_ventas = float(df_trece[\"ventas\"].median())\n",
    "# mediana_ventas\n",
    "\n",
    "## âœ… Calculamos NormalizaciÃ³n robusta\n",
    "df_trece[\"ventas_robust\"] = ((df_trece[\"ventas\"] - mediana_ventas) / iqr_ventas).round(2)\n",
    "df_trece.head()\n",
    "\n",
    "## ğŸ’¡ La normalizaciÃ³n robusta es una tÃ©cnica de preprocesamiento de datos\n",
    "##    que escala las caracterÃ­sticas utilizando estadÃ­sticas que son menos\n",
    "##    sensibles a valores atÃ­picos (outliers) que la normalizaciÃ³n estÃ¡ndar.\n",
    "\n",
    "\"\"\"\n",
    "14. ExtracciÃ³n de partes avanzadas de fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae la hora, el minuto y el nombre del dÃ­a de la semana.\n",
    "âœï¸ Resultado esperado: Tres nuevas columnas: hora, minuto, dia_semana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12 14:35:00\",\"2021-05-12 20:10:00\",\"2021-05-13 08:45:00\"]\n",
    "}\n",
    "\n",
    "df_catorce = pd.DataFrame(diccionario_catorce)\n",
    "# df_catorce.head()\n",
    "df_catorce[\"fecha\"] = pd.to_datetime(df_catorce[\"fecha\"])\n",
    "# df_catorce.head()\n",
    "df_catorce[\"hora\"] = df_catorce[\"fecha\"].dt.hour\n",
    "df_catorce[\"minuto\"] = df_catorce[\"fecha\"].dt.minute\n",
    "df_catorce[\"dia_semana\"] = df_catorce[\"fecha\"].dt.day_name(locale=\"\")\n",
    "df_catorce.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "15. ExtracciÃ³n de features de texto (tokens Ãºnicos)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras Ãºnicas en cada comentario.\n",
    "âœï¸ Resultado esperado: Columna palabras_unicas con valores (3, 2, 4).\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_quince = {\n",
    "    \"comentario\":[\"Muy buen buen servicio\",\"Servicio aceptable aceptable\",\"No me gustÃ³ el servicio\"]\n",
    "}\n",
    "df_quince = pd.DataFrame(diccionario_quince)\n",
    "df_quince[\"palabras_unicas\"] = df_quince[\"comentario\"].apply(lambda x:len(set(x.split(\" \"))))\n",
    "df_quince.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873079e7",
   "metadata": {},
   "source": [
    "###### POLARS ğŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ad4d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comentario</th><th>palabras_unicas</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Muy buen buen servicio&quot;</td><td>3</td></tr><tr><td>&quot;Servicio aceptable aceptable&quot;</td><td>2</td></tr><tr><td>&quot;No me gustÃ³ el servicio&quot;</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comentario                   â”† palabras_unicas â”‚\n",
       "â”‚ ---                          â”† ---             â”‚\n",
       "â”‚ str                          â”† i64             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Muy buen buen servicio       â”† 3               â”‚\n",
       "â”‚ Servicio aceptable aceptable â”† 2               â”‚\n",
       "â”‚ No me gustÃ³ el servicio      â”† 5               â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\"\"\"\n",
    "11. Variables categÃ³ricas cruzadas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Titanic\n",
    "ğŸ—’ï¸ Enunciado: Crea una nueva variable categÃ³rica que combine pclass y sex (ejemplo: \"1_female\", \"3_male\").\n",
    "âœï¸ Resultado esperado: Columna clase_sexo con las combinaciones Ãºnicas.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_once = pl.read_csv(\"../datasets/titanic.csv\",separator=\",\")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Creamos la variable que permita las combinaciones de datos en ambas columnas.\n",
    "df_once = df_once.with_columns(\n",
    "    (pl.col(\"pclass\").cast(pl.String)+\"-\"+pl.col(\"sex\")).alias(\"clase_sexo\")\n",
    ")\n",
    "# df_once.head()\n",
    "\n",
    "## -- âœ… Categorizamos las variables de esa columna.\n",
    "dummies_pclass_sex = df_once.select(pl.col(\"clase_sexo\")).to_dummies(columns=[\"clase_sexo\"])\n",
    "dummies_pclass_sex\n",
    "\n",
    "## -- âœ… Unificamos al dataframe original\n",
    "df_once_final = pl.concat([df_once,dummies_pclass_sex],how=\"horizontal\")\n",
    "df_once_final.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "12. Binning desigual basado en reglas\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Tips\n",
    "ğŸ—’ï¸ Enunciado: Clasifica tip en categorÃ­as: \"Bajo\" (<2), \"Medio\" (2 - 5), \"Alto\" (>5).\n",
    "âœï¸ Resultado esperado: Nueva columna tip_categoria con esas 3 categorÃ­as.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "df_doce = pl.read_csv(\"../datasets/tips.csv\",separator=\",\")\n",
    "# df_doce.head()\n",
    "df_doce = df_doce.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"tip\")<2\n",
    "    ).then(pl.lit(\"Bajo\"))\n",
    "    .when(\n",
    "        (pl.col(\"tip\")>=2) & (pl.col(\"tip\")<=5)\n",
    "    ).then(pl.lit(\"Medio\"))\n",
    "    .otherwise(pl.lit(\"Alto\"))\n",
    ")\n",
    "df_doce.head()\n",
    "\n",
    "\"\"\"\n",
    "13. NormalizaciÃ³n robusta\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Aplica una normalizaciÃ³n robusta (usando mediana e IQR).\n",
    "âœï¸ Resultado esperado: Columna ventas_robust que reduzca la influencia de outliers.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_trece = {\n",
    "    \"producto\":[\"A\",\"B\",\"C\",\"D\"],\n",
    "    \"ventas\":[10,200,1000,10000]\n",
    "}\n",
    "df_trece = pl.DataFrame(diccionario_trece)\n",
    "# df_trece.head()\n",
    "\n",
    "##  âœ… Hallamos los Quartiles (columna ventas)\n",
    "q1_ventas = df_trece[\"ventas\"].quantile(0.25)\n",
    "# q1_ventas\n",
    "q3_ventas = df_trece[\"ventas\"].quantile(0.75)\n",
    "# q3_ventas\n",
    "\n",
    "## âœ… Hallamos IQR (Rango Interquartil - columnas ventas)\n",
    "iqr_ventas = q3_ventas - q1_ventas\n",
    "# iqr_ventas\n",
    "\n",
    "## âœ… Hallamos mediana (columna ventas)\n",
    "mediana_ventas = float(df_trece[\"ventas\"].median())\n",
    "# mediana_ventas\n",
    "\n",
    "## âœ… Calculamos NormalizaciÃ³n robusta\n",
    "df_trece = df_trece.with_columns(\n",
    "    ((pl.col(\"ventas\") - mediana_ventas)/iqr_ventas).round(2).alias(\"ventas_robust\")\n",
    ")\n",
    "df_trece.head()\n",
    "\n",
    "## ğŸ’¡ La normalizaciÃ³n robusta es una tÃ©cnica de preprocesamiento de datos\n",
    "##    que escala las caracterÃ­sticas utilizando estadÃ­sticas que son menos\n",
    "##    sensibles a valores atÃ­picos (outliers) que la normalizaciÃ³n estÃ¡ndar.\n",
    "\n",
    "\"\"\"\n",
    "14. ExtracciÃ³n de partes avanzadas de fecha\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Extrae la hora, el minuto y el nombre del dÃ­a de la semana.\n",
    "âœï¸ Resultado esperado: Tres nuevas columnas: hora, minuto, dia_semana.\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_catorce = {\n",
    "    \"id\":[1,2,3],\n",
    "    \"fecha\":[\"2021-05-12 14:35:00\",\"2021-05-12 20:10:00\",\"2021-05-13 08:45:00\"]\n",
    "}\n",
    "\n",
    "df_catorce = pl.DataFrame(diccionario_catorce)\n",
    "# df_catorce.head()\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    pl.col(\"fecha\").str.to_datetime().alias(\"fecha\")\n",
    ")\n",
    "df_catorce = df_catorce.with_columns(\n",
    "    pl.col(\"fecha\").dt.hour().alias(\"hora\"),\n",
    "    pl.col(\"fecha\").dt.minute().alias(\"minuto\"),\n",
    "    pl.col(\"fecha\").dt.strftime(format=\"%A\").alias(\"dia_semana\")\n",
    ")\n",
    "df_catorce.head()\n",
    "\n",
    "\"\"\"\n",
    "15. ExtracciÃ³n de features de texto (tokens Ãºnicos)\n",
    "\n",
    "ğŸ—ƒï¸ Dataset: Diccionario\n",
    "ğŸ—’ï¸ Enunciado: Crea una columna con el nÃºmero de palabras Ãºnicas en cada comentario.\n",
    "âœï¸ Resultado esperado: Columna palabras_unicas con valores (3, 2, 4).\n",
    "\n",
    "\"\"\"\n",
    "## âœ”ï¸ SoluciÃ³n\n",
    "diccionario_quince = {\n",
    "    \"comentario\":[\"Muy buen buen servicio\",\"Servicio aceptable aceptable\",\"No me gustÃ³ el servicio\"]\n",
    "}\n",
    "df_quince = pl.DataFrame(diccionario_quince)\n",
    "df_quince = df_quince.with_columns(\n",
    "    pl.col(\"comentario\").map_elements(lambda x:len(set(x.split(\" \"))),return_dtype=pl.Int64).alias(\"palabras_unicas\")\n",
    ")\n",
    "df_quince.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
